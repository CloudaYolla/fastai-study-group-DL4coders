{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HBA PA4\n",
    "\n",
    "French version: https://github.com/Daniel-R-Armstrong/French-Wiki-2500-Pretrained-SentencePiece-LM/blob/master/fr_spm.ipynb\n",
    "https://forums.fast.ai/t/sentencepiece/53010/50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turkish ULMFiT from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #hide\n",
    "# !pip install -Uqq fastbook\n",
    "# import fastbook\n",
    "# fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install fastai==1.0.61 #This right here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install fastcore==1. #This right here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #fastai v1 API: https://fastai1.fast.ai/overview.html\n",
    "# from fastai.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai.utils import *\n",
    "# show_install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from fastai import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print ('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('/home/ec2-user/.fastai/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=128\n",
    "torch.cuda.set_device(2)\n",
    "# data_path = Config.data_path()\n",
    "# data_path = '/home/ec2-user/.fastai/data'\n",
    "\n",
    "lang = 'tx'\n",
    "name = f'{lang}wiki'\n",
    "path = data_path/name\n",
    "path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_path = path/'models'\n",
    "mdl_path.mkdir(exist_ok=True)\n",
    "lm_fns = [mdl_path/f'{lang}_wt', mdl_path/f'{lang}_wt_vocab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ec2-user/.fastai/data/txwiki')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tx'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turkish wikipedia model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #HBA\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install wikiextractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wikiextractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlputils import split_wiki,get_wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncompress wiki file from cli\n",
    "\n",
    "Source: https://github.com/attardi/wikiextractor\n",
    "\n",
    "1. Do `python setup.py install`\n",
    "1. Double check with: `which wikiextractor`\n",
    "\n",
    "`\n",
    "base) [ec2-user@ip-172-16-69-8 trwiki]$ which wikiextractor\n",
    "~/anaconda3/bin/wikiextractor\n",
    "`\n",
    "\n",
    "1. Uncrompress\n",
    "\n",
    "`\n",
    "(base) [ec2-user@ip-172-16-69-8 trwiki]$ python -m wikiextractor.WikiExtractor trwiki-latest-pages-articles.xml\n",
    "INFO: 2832129   Mucize Dünyası: New York – Birleşik Kahramanlar\n",
    "INFO: 2832111   Kraliyet Mühendislik Akademisi üyesi kadınlar listesi\n",
    "INFO: 2832131   Ilana Rovina\n",
    "INFO: 2832155   Artur Taymazov\n",
    "INFO: 2832130   Enka (kadın voleybol takımı) 1992-93 sezonu\n",
    "INFO: Finished 31-process extraction of 377951 articles in 123.1s (3071.5 art/s)\n",
    "INFO: total of page: 732729, total of articl page: 377951; total of used articl page: 377951\n",
    "(base) [ec2-user@ip-172-16-69-8 trwiki]$ which wikiextractor\n",
    "~/anaconda3/bin/wikiextractor\n",
    "`\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading...\n",
      "unzipping...\n",
      "extracting... akirmak\n"
     ]
    }
   ],
   "source": [
    "get_wiki(path,lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<doc id=\"10\" url=\"https://tr.wikipedia.org/wiki?curid=10\" title=\"Cengiz Han\">\r\n",
      "Cengiz Han\r\n",
      "\r\n",
      "Cengiz Han (doğum Temuçin, – 18 Ağustos 1227) ölümünden sonra tarihin en büyük yüzölçümüne sahip imparatorluğu haline gelmiş Moğol İmparatorluğu'nun kurucusu ve ilk Kağanı ve İmparatoru olan Moğol komutan ve hükümdar. 13. Yüzyılın başında Orta Asya'daki tüm göçebe bozkır kavimlerini birleştirerek bir ulus hâline getirdi ve o ulusu \"Moğol\" siyasi kimliği çatısı altında topladı.\r\n"
     ]
    }
   ],
   "source": [
    "# from nlputils import split_wiki,get_wiki\n",
    "\n",
    "# get_wiki(path,lang)\n",
    "!head -n4 {path}/{name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/ec2-user/.fastai/data/txwiki/wikiextractor-orig'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/txwiki-latest-pages-articles.xml'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/txwiki-latest-pages-articles.xml.bz2'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/log'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/wikiextractor'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/docs'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/models'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/txwiki')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.fastai/data/txwiki/docs already exists; not splitting\n"
     ]
    }
   ],
   "source": [
    "dest = split_wiki(path,lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/ec2-user/.fastai/data/txwiki/docs/Kara delik.txt'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/docs/Heaviside katmanı.txt'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/docs/Zurna.txt'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/docs/Özallı Yıllar.txt'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/docs/Kuşça, Cihanbeyli.txt')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest.ls()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ec2-user/.fastai/data/txwiki/docs')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turkish is an [Agglutinative_language](https://en.wikipedia.org/wiki/Agglutinative_language) so it needs special care!\n",
    "\n",
    "![Turkish morphemes example](images/turkish.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HBA fix\n",
    "\n",
    "1. Open via: `vi ~/SageMaker/.env/fastai/lib/python3.6/site-packages/fastai/text/data.py`\n",
    "1. see: https://forums.fast.ai/t/multifit-runtime-error-permission-denied/72874/5\n",
    "1. See: https://github.com/fastai/fastai/issues/2572 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ec2-user/.fastai/data/txwiki/docs')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/ec2-user/.fastai/data/txwiki/docs/Kara delik.txt'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/docs/Heaviside katmanı.txt'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/docs/Zurna.txt'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/docs/Özallı Yıllar.txt'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/docs/Kuşça, Cihanbeyli.txt')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest.ls()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56219\r\n"
     ]
    }
   ],
   "source": [
    "#PRINT NUMBER OF FILES IN DIR\n",
    "!find /home/ec2-user/.fastai/data/txwiki/docs -mindepth 1 -maxdepth 1 -printf '.' | wc -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE ALL BUT 1O FILES\n",
    "# !ls -1tr /home/ec2-user/.fastai/data/txwiki/docs | head -n -100 | xargs -d '\\n' rm -f --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # x.sh\n",
    "# cp \"Zurna.txt\" ../docs-100/\n",
    "# cp \"Ulucak Höyüğü.txt\" ../docs-100/\n",
    "# cp \"Özallı Yıllar.txt\" ../docs-100/\n",
    "# cp \"Kuşça, Cihanbeyli.txt\" ../docs-100/\n",
    "# cp \"Kara delik.txt\" ../docs-100/\n",
    "# cp \"Heaviside katmanı.txt\" ../docs-100/\n",
    "# cp \"Çağlar Söyüncü.txt\" ../docs-100/\n",
    "# cp \"Bilim ve Teknik.txt\" ../docs-100/\n",
    "# cp \"Arş.txt\" ../docs-100/\n",
    "# cp \"Amnesia: Dark Descent.txt\" ../docs-100/\n",
    "# cp \"Yunancanın romanizasyonu.txt\" ../docs-100/\n",
    "# cp \"Yön Tanrıları.txt\" ../docs-100/\n",
    "# cp \"Yemeksepeti.com.txt\" ../docs-100/\n",
    "# cp \"Yaman, Mudanya.txt\" ../docs-100/\n",
    "# cp \"WWE 24_7 Championship.txt\" ../docs-100/\n",
    "# cp \"Wilhelm Reich.txt\" ../docs-100/\n",
    "# cp \"Web grafik tasarımı.txt\" ../docs-100/\n",
    "# cp \"Vuslat Doğan Sabancı.txt\" ../docs-100/\n",
    "# cp \"Vladislav Tretyak.txt\" ../docs-100/\n",
    "# cp \"VIII. Henry.txt\" ../docs-100/\n",
    "# cp \"Us (The Walking Dead).txt\" ../docs-100/\n",
    "# cp \"USS Arizona (BB-39).txt\" ../docs-100/\n",
    "# cp \"Türk Kurtuluş Savaşı sırasında katliamlar.txt\" ../docs-100/\n",
    "# cp \"TRT 1.txt\" ../docs-100/\n",
    "# cp \"Transvaal Cumhuriyeti.txt\" ../docs-100/\n",
    "# cp \"Trabzon Konferansı.txt\" ../docs-100/\n",
    "# cp \"Today Was a Fairytale.txt\" ../docs-100/\n",
    "# cp \"Tehlikeli Oyun (film, 2008).txt\" ../docs-100/\n",
    "# cp \"TCG Berk.txt\" ../docs-100/\n",
    "# cp \"Taziz.txt\" ../docs-100/\n",
    "# cp \"Taye Taiwo.txt\" ../docs-100/\n",
    "# cp \"Tatyana Volosojar.txt\" ../docs-100/\n",
    "# cp \"Tanzanya arması.txt\" ../docs-100/\n",
    "# cp \"Süleyman Türkünal.txt\" ../docs-100/\n",
    "# cp \"Street Fighter (video oyunu).txt\" ../docs-100/\n",
    "# cp \"Sons of Anarchy.txt\" ../docs-100/\n",
    "# cp \"Sol SR.txt\" ../docs-100/\n",
    "# cp \"Sloven İstihbarat ve Güvenlik Ajansı.txt\" ../docs-100/\n",
    "# cp \"Simon bar Kohba.txt\" ../docs-100/\n",
    "# cp \"Silverline Ankastre.txt\" ../docs-100/\n",
    "# cp \"Sıçansılar (balık).txt\" ../docs-100/\n",
    "# cp \"Shim Changmin.txt\" ../docs-100/\n",
    "# cp \"Seş.txt\" ../docs-100/\n",
    "# cp \"Şar Dağları.txt\" ../docs-100/\n",
    "# cp \"San Marino millî futbol takımı.txt\" ../docs-100/\n",
    "# cp \"Saga Norén.txt\" ../docs-100/\n",
    "# cp \"Robot of Sherwood.txt\" ../docs-100/\n",
    "# cp \"Refik Turan.txt\" ../docs-100/\n",
    "# cp \"RaceRoom.txt\" ../docs-100/\n",
    "# cp \"Porsche 911 GT2.txt\" ../docs-100/\n",
    "# cp \"Piçan İlçesi.txt\" ../docs-100/\n",
    "# cp \"Perşa Liha.txt\" ../docs-100/\n",
    "# cp \"Perpa.txt\" ../docs-100/\n",
    "# cp \"Pepsi Twist.txt\" ../docs-100/\n",
    "# cp \"Outlook Express.txt\" ../docs-100/\n",
    "# cp \"Öter ardıç kuşu.txt\" ../docs-100/\n",
    "# cp \"Østfold.txt\" ../docs-100/\n",
    "# cp \"Ortak varlıkların trajedisi.txt\" ../docs-100/\n",
    "# cp \"Ondokuz Mayıs Üniversitesi.txt\" ../docs-100/\n",
    "# cp \"Ömer Dinçer.txt\" ../docs-100/\n",
    "# cp \"Nissim Kamondo.txt\" ../docs-100/\n",
    "# cp \"Nif geni.txt\" ../docs-100/\n",
    "# cp \"Nick Hornby.txt\" ../docs-100/\n",
    "# cp \"Neo Geo.txt\" ../docs-100/\n",
    "# cp \"NBA Yılın En İyi Beşi.txt\" ../docs-100/\n",
    "# cp \"Nancy Atakan.txt\" ../docs-100/\n",
    "# cp \"Nacho Vidal.txt\" ../docs-100/\n",
    "# cp \"Mukor.txt\" ../docs-100/\n",
    "# cp \"Miuccia Prada.txt\" ../docs-100/\n",
    "# cp \"Millî marş.txt\" ../docs-100/\n",
    "# cp \"Millet İttifakı.txt\" ../docs-100/\n",
    "# cp \"Mikio Oda.txt\" ../docs-100/\n",
    "# cp \"Matvey Muranov.txt\" ../docs-100/\n",
    "# cp \"Maliye Bakanlığı (Azerbaycan).txt\" ../docs-100/\n",
    "# cp \"Malabadi Köprüsü.txt\" ../docs-100/\n",
    "# cp \"Makas Eller.txt\" ../docs-100/\n",
    "# cp \"Lund Üniversitesi.txt\" ../docs-100/\n",
    "# cp \"Liemba.txt\" ../docs-100/\n",
    "# cp \"LA X.txt\" ../docs-100/\n",
    "# cp \"Lando Buzzanca.txt\" ../docs-100/\n",
    "# cp \"Kutsal Duman.txt\" ../docs-100/\n",
    "# cp \"Kurtlar Vadisi bölümleri listesi.txt\" ../docs-100/\n",
    "# cp \"Kuch Kuch Hota Hai.txt\" ../docs-100/\n",
    "# cp \"Koyulhisar.txt\" ../docs-100/\n",
    "# cp \"Kökpar.txt\" ../docs-100/\n",
    "# cp \"Kitap tasarımı.txt\" ../docs-100/\n",
    "# cp \"Kelime.txt\" ../docs-100/\n",
    "# cp \"Kazatomprom.txt\" ../docs-100/\n",
    "# cp \"Kayseri'de 2009 Türkiye yerel seçimleri.txt\" ../docs-100/\n",
    "# cp \"Karpal tünel sendromu.txt\" ../docs-100/\n",
    "# cp \"Kanserojen.txt\" ../docs-100/\n",
    "# cp \"Joseph Henry.txt\" ../docs-100/\n",
    "# cp \"José Bosingwa.txt\" ../docs-100/\n",
    "# cp \"John 5.txt\" ../docs-100/\n",
    "# cp \"Je suis Charlie.txt\" ../docs-100/\n",
    "# cp \"Jesse Ventura.txt\" ../docs-100/\n",
    "# cp \"Jérôme Rothen.txt\" ../docs-100/\n",
    "# cp \"Japon militarizmi.txt\" ../docs-100/\n",
    "# cp \"Iustinus.txt\" ../docs-100/\n",
    "# cp \"İstihbarat Bakanlığı (İran).txt\" ../docs-100/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\r\n"
     ]
    }
   ],
   "source": [
    "#PRINT NUMBER OF FILES IN DIR\n",
    "!find /home/ec2-user/.fastai/data/txwiki/docs -mindepth 1 -maxdepth 1 -printf '.' | wc -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/fastai/text/data.py:476: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  ds.items = np.array(sum(e.map(self._encode_batch, partition_by_cores(ds.items, self.n_cpus)), []))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(5736, 90)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = (TextList.from_folder(dest, processor=[OpenFileProcessor(), SPProcessor()])\n",
    "        .split_by_rand_pct(0.1, seed=42)\n",
    "        .label_for_lm()\n",
    "        .databunch(bs=bs, num_workers=1))\n",
    "\n",
    "data.save(f'{lang}_databunch')\n",
    "len(data.vocab.itos),len(data.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(dest, f'{lang}_databunch', bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/fastai/text/data.py:339: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  idx_min = (t != self.pad_idx).nonzero().min()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>. ▁yükseklik ten ▁sonra ▁başlar . ▁xxmaj ▁bu ▁ismi n ▁veril mesi nin ▁sebebi ▁i yon osfer ▁bölgesinde ▁güneş ▁rüzg ar ı ▁ve ▁ko z mik ▁ ışık ▁gibi ▁uzay ▁kaynak lı ▁etkiler den ▁ötürü ▁i yon ▁yoğunluğu nun ▁yüksek ▁oluş u dur . ▁xxmaj ▁bu ▁i yon lar ▁çeşitli ▁ara ▁katman lara ▁yol ▁açar . ▁xxmaj ▁bilim ▁dünya sında ▁bu ▁ara ▁katman lar ▁xxup ▁ d , e ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>▁ar x e oloji ▁kazı n tılar ▁zaman ı ▁mar al ▁ boynuz un dan ▁hazırlanmış ▁dört ▁adet ▁zurna ▁ aş kar ▁edilmişti r . ▁xxmaj ▁bilim ▁adam larının ▁hesap lama larına ▁göre , ▁yüksek ▁ ze v k le ▁yapılmış ▁bu ▁çalgı ların ▁üç ▁bin ▁yıl ▁yaş ı ▁vardır . ▁xxmaj ▁esasen ▁erik , ▁ceviz , ▁ ya ba nı ▁ s öğüt ▁ve ▁ du t ▁ağa c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>▁xxmaj ▁demir el ' in ▁tekrar ▁başbakan ▁olması , ▁xxmaj ▁özal ' ın ▁hem ▁xxmaj ▁demir el ▁hem ▁de ▁xxmaj ▁yılmaz ▁ile ▁anlaşma z lığı , ▁xxmaj ▁özal ' ın ▁anap ' tan ▁ko p u şu , ▁yeni ▁parti ▁ arayışlar ı ▁ve ▁ ölümüne ▁yer ▁verildi . ▁&lt; ▁/ ▁ doc &gt; ▁xxbos ▁xxmaj ▁kuşça , ▁xxmaj ▁cihanbeyli ▁xxmaj ▁kuşca ▁( k ür t çe : ▁xxmaj ▁he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7 ▁cm ) ö l ç ü lerinde ▁basıl maya ▁başlan dı . ▁2001 ▁yılında ▁xxmaj ▁i ̇ r f an ▁xxmaj ▁say ar ' ın ▁çiz diği ▁xxmaj ▁ zi h ni ▁xxmaj ▁sinir ' in ▁\" p ro ce lerini \" ▁yayınla maya ▁başladı . ▁xxmaj ▁bu ▁yıl da ▁400 . ▁sayısında ▁( mar t ▁2001 ) ▁birlikte ▁ek ▁olarak ▁\" i ̇ nsan ▁xxmaj ▁genomu \" ▁verildi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>▁sonra ▁xxmaj ▁daniel ▁bir ▁ ışık ▁görür ▁ve ▁xxmaj ▁ag rip pa ' nın ▁sesi ▁duyul ur ; ▁xxmaj ▁w e ye r ' a ▁xxmaj ▁daniel ' ı ▁kurtar mak ▁için ▁bir ▁şey ▁yap masını ▁söyler ▁ve ▁xxmaj ▁daniel ▁hayat a ▁geri ▁döner . ▁&lt; br &gt; ▁xxmaj ▁gizli ▁xxmaj ▁son : ▁xxmaj ▁alexander , ▁xxmaj ▁daniel ' ı ▁hücre ye ▁atar ▁ve ▁ona ▁bekle mesi ni ▁söyler .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn = language_model_learner(data, AWD_LSTM, drop_mult=0.1, wd=0.1, pretrained=False).to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HBA French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='6' class='' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      85.71% [6/7 01:05<00:10]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.641625</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.640193</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.640217</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.640306</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.640304</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6.641004</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3' class='' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      18.75% [3/16 00:02<00:08 6.6389]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find(stop_div=False, num_it=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min numerical gradient: 8.32E-06\n",
      "Min loss divided by 10: 1.32E-01\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-3\n",
    "lr *= bs/48  # Scale learning rate by batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7.656495</td>\n",
       "      <td>6.846730</td>\n",
       "      <td>0.056994</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.206811</td>\n",
       "      <td>7.901802</td>\n",
       "      <td>0.038951</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.146518</td>\n",
       "      <td>6.850193</td>\n",
       "      <td>0.056994</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.994226</td>\n",
       "      <td>6.723795</td>\n",
       "      <td>0.056994</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.892519</td>\n",
       "      <td>6.729263</td>\n",
       "      <td>0.056994</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6.824438</td>\n",
       "      <td>6.711360</td>\n",
       "      <td>0.056994</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6.777204</td>\n",
       "      <td>6.704089</td>\n",
       "      <td>0.056994</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>6.742869</td>\n",
       "      <td>6.696327</td>\n",
       "      <td>0.056994</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>6.714705</td>\n",
       "      <td>6.692200</td>\n",
       "      <td>0.056994</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>6.693724</td>\n",
       "      <td>6.691544</td>\n",
       "      <td>0.056994</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n"
     ]
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(10, lr, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.to_fp32().save(lm_fns[0], with_opt=False)\n",
    "learn.data.vocab.save(lm_fns[1].with_suffix('.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.to_fp32().export('/home/ec2-user/.fastai/data/txwiki/models/learner_modtx_spm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data.vocab.save('/home/ec2-user/.fastai/data/txwiki/models/learner_vocab_tx_spm.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('/home/ec2-user/.fastai/data/txwiki/models/learner_tx_spm_enc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ec2-user/.fastai/data/txwiki/models/learner_mod_tx_spm_save.pkl.pth')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.to_fp32().save('/home/ec2-user/.fastai/data/txwiki/models/learner_mod_tx_spm_save.pkl',return_path=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From NN-IMDB NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5736, 90)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.vocab.itos),len(data.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>▁xxbos ▁xxmaj ▁i ̇ stihbarat ▁xxmaj ▁bakanlığı ▁( i ̇ ran ) ▁xxmaj ▁i ̇ ran ▁xxmaj ▁i ̇ slam ▁xxmaj ▁cumhuriyeti ▁xxmaj ▁i ̇ stihbarat ▁xxmaj ▁bakanlığı ▁( farsça : ▁ و ِ ز ا ر َ ت ِ ▁ ا ِ ط ّ لا ع ا ت ▁ ج ُ م ه و ر ی ِ ▁ ا ِ س لا م ی ِ ▁ ا ی ر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>▁ shi m ▁xxmaj ▁ ji - ye on ▁adında ▁iki ▁kız ▁kardeşi ▁vardır . ▁xxup ▁sm ▁xxmaj ▁ entertainment ’ a ▁ba d m in ton ▁oynar ken ▁bir ▁kadın ın ▁ona ▁yaklaşı p ▁yarışmalar a ▁gelmesi ni ▁söyleme sinden ▁sonra ▁annesi ▁istedi ği ▁için ▁katıl mıştır . ▁xxup ▁sm ▁xxmaj ▁ entertainment ▁tarafından ▁düzenlenen ▁xxmaj ▁altıncı ▁xxmaj ▁gençlik ▁xxmaj ▁yarışma sında ▁en ▁iyi ▁şarkıcı ▁ve ▁en ▁iyi ▁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>\", ▁ genellikle ▁sürdürüle bilir ▁kalkınma , ▁ekonomik ▁büyüme ▁ve ▁çevre yi ▁koruma nın ▁birbirin e ▁bağlan ması , ▁ve ▁küresel ▁ıs ın ma ▁tartışmaları yla ▁bağlantılı ▁olarak ▁bahsedil ir . ▁xxmaj ▁ayn ı ▁zamanda ▁ekonomi , ▁ evrim sel ▁psikoloji , ▁ an tr op oloji , ▁oyun ▁teori si , ▁siyaset , ▁vergile ndirme ▁ve ▁so sy oloji ▁alanlarında ki ▁davranış ları ▁analiz ▁etmek ▁için ▁de ▁kullanıl mıştır</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>den ▁etkilen ir ▁ayrıca ▁kediler ▁ve ▁diğer ▁ yı r t ıcı ▁kuşlar ▁tarafından ▁avlan ırlar . ▁xxmaj ▁öte r ▁ardı ç ▁kuşu ▁18 31 ▁yılında ▁xxmaj ▁alman ▁ or ni t olog ▁xxmaj ▁christ ian ▁xxmaj ▁ lu d w i g ▁xxmaj ▁ bre h m ▁tarafından ▁tanıml an mıştır ▁ve ▁o ▁zamanda n ▁beri ▁ilk ▁verilen ▁bilim sel ▁adını ▁koruma ktadır : ▁\" turdus ▁ philo melos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>▁xxmaj ▁bu ▁şakıma , ▁xxmaj ▁dış ▁xxmaj ▁hebrid ler ▁alt ▁türü ▁tarafından ▁şubat ▁ayında n ▁haziran ▁ay ına ▁kadar ▁yapılır ▁ancak ▁diğer ▁alt ▁tür ler ▁tarafından ▁kasım dan ▁temmuz a ▁kadar ▁yapılır . ▁xxmaj ▁ağırlığı na ▁ora n la ▁bu ▁kuş ▁türü ▁en ▁yüksek ▁ses le ▁şakı yan ▁kuşlar dan ▁biridir . ▁xxmaj ▁erkek ▁birey lerin ▁100 ' den ▁fazla ▁müzik al ▁dizi ▁içeren ▁re per tu var ları ▁olabilir</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = data.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4646"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.stoi[\"stratejik\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_words = set(vocab.itos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stratejik',\n",
       " '▁logosu',\n",
       " '▁çıkarak',\n",
       " '▁bastırı',\n",
       " '▁için',\n",
       " 'talya',\n",
       " 'berk',\n",
       " 'meye',\n",
       " '▁buluntular',\n",
       " 'ttiği',\n",
       " '▁sherwood',\n",
       " 'ların',\n",
       " '▁yorumcu',\n",
       " ']',\n",
       " '▁kutusunu',\n",
       " '▁birlik',\n",
       " '▁xxpad',\n",
       " 'tor',\n",
       " '▁güreşçi',\n",
       " '▁şerefine',\n",
       " '▁çalışma',\n",
       " '▁2016',\n",
       " '▁demokrat',\n",
       " 'keep',\n",
       " '▁yaz',\n",
       " '▁bölgesel',\n",
       " '▁höyük',\n",
       " '▁şimdi',\n",
       " '▁zelanda',\n",
       " '▁türk',\n",
       " '▁yer',\n",
       " 'oksit',\n",
       " '▁rudo',\n",
       " 'e',\n",
       " '▁geliştiren',\n",
       " '▁orantı',\n",
       " '▁rusya',\n",
       " '▁gelmiştir',\n",
       " '▁adı',\n",
       " '▁onur',\n",
       " '▁boy',\n",
       " '▁west',\n",
       " '▁veriler',\n",
       " '▁kemeri',\n",
       " '▁başkanlığı',\n",
       " '▁başın',\n",
       " '▁fark',\n",
       " '▁rauf',\n",
       " 'r',\n",
       " '▁tanrısı',\n",
       " 'doğal',\n",
       " 'pati',\n",
       " '▁%7',\n",
       " '▁tür',\n",
       " '▁dizi',\n",
       " '▁yazılım',\n",
       " '▁doğası',\n",
       " 'aptı',\n",
       " '▁töreninde',\n",
       " 'υ',\n",
       " '▁simülasyon',\n",
       " 'ar',\n",
       " '▁idi',\n",
       " '▁kruvazörü',\n",
       " '▁san',\n",
       " '▁harf',\n",
       " 'ül',\n",
       " 'dy',\n",
       " '▁sızdırma',\n",
       " '▁melissa',\n",
       " 'ması',\n",
       " '▁devletin',\n",
       " '1902',\n",
       " '▁algıla',\n",
       " '▁kaza',\n",
       " '▁\"',\n",
       " '▁ilerleme',\n",
       " 'yalnız',\n",
       " '▁görmek',\n",
       " '▁raporu',\n",
       " 'lenmiştir',\n",
       " 'yalı',\n",
       " '▁taret',\n",
       " '▁hükûmette',\n",
       " 'yu',\n",
       " '▁etrafında',\n",
       " '▁ordu',\n",
       " '▁callao',\n",
       " '▁bölümden',\n",
       " 'tal',\n",
       " '▁kupa',\n",
       " '▁doldurma',\n",
       " 'gazi',\n",
       " 'likle',\n",
       " 'с',\n",
       " '▁gitarist',\n",
       " 'lan',\n",
       " '▁ergen',\n",
       " '▁vardı',\n",
       " '...\"',\n",
       " '▁hem',\n",
       " 'homa',\n",
       " '▁suçlu',\n",
       " '▁kuşatıldı',\n",
       " '▁denetleme',\n",
       " '▁doçent',\n",
       " '▁ailesi',\n",
       " '▁bulun',\n",
       " 'ynyrd',\n",
       " '▁akarsu',\n",
       " '▁elektrofil',\n",
       " '▁siklus',\n",
       " '▁ayrıldı',\n",
       " '▁corne',\n",
       " '▁güney',\n",
       " '▁manastır',\n",
       " '▁bölümde',\n",
       " '▁bi',\n",
       " '▁miniskop',\n",
       " '▁ka',\n",
       " '▁pepsi',\n",
       " '▁uygulaya',\n",
       " '190',\n",
       " '▁kö',\n",
       " '▁henrik',\n",
       " '▁sahiptir',\n",
       " '▁tersanesi',\n",
       " '▁en',\n",
       " '▁söylemleri',\n",
       " '▁verir',\n",
       " '▁filmin',\n",
       " 'love',\n",
       " '▁adıyaman',\n",
       " '▁sert',\n",
       " 'polis',\n",
       " '▁kongresinde',\n",
       " 'düğü',\n",
       " '▁çalışmalarını',\n",
       " '▁dinçer',\n",
       " '>',\n",
       " '▁450',\n",
       " '▁han',\n",
       " '▁civar',\n",
       " '▁tra',\n",
       " '▁bak',\n",
       " '▁nichol',\n",
       " '▁müziği',\n",
       " '▁akraba',\n",
       " '▁birçoğu',\n",
       " '7',\n",
       " '▁edildi',\n",
       " '▁bakteride',\n",
       " '▁onursal',\n",
       " '▁bileşenlerin',\n",
       " '▁kanal',\n",
       " '▁moğolcada',\n",
       " '▁ilişkilerini',\n",
       " '▁vii',\n",
       " '▁filmde',\n",
       " '▁bozuklu',\n",
       " '▁itibarıyla',\n",
       " '▁kidd',\n",
       " '▁filosu',\n",
       " '40',\n",
       " 'lenerek',\n",
       " '▁kav',\n",
       " '▁cumhuriyet',\n",
       " '▁kür',\n",
       " '▁yaptığını',\n",
       " '▁market',\n",
       " '▁adlı',\n",
       " '▁yapma',\n",
       " '▁dryden',\n",
       " '▁devredilmişti',\n",
       " '▁uyar',\n",
       " '▁kanalın',\n",
       " '▁5,',\n",
       " '▁metot',\n",
       " '▁tanıml',\n",
       " '▁vi',\n",
       " 'ph',\n",
       " '4\"',\n",
       " '▁allah',\n",
       " '▁konsolosu',\n",
       " 'müşavir',\n",
       " 'miştir',\n",
       " '▁gizli',\n",
       " '▁orman',\n",
       " '▁ba',\n",
       " '▁kuyruk',\n",
       " 'gil',\n",
       " 'carol',\n",
       " '▁christ',\n",
       " 'sinde',\n",
       " '▁salyangozlar',\n",
       " 'çindekiler',\n",
       " '▁soruşturma',\n",
       " '▁üçlü',\n",
       " '▁kapak',\n",
       " 'futbol',\n",
       " 'biliyor',\n",
       " 'stre',\n",
       " '▁morozov',\n",
       " '▁doktorasın',\n",
       " '▁toronto',\n",
       " '▁dalgasını',\n",
       " '▁ateş',\n",
       " '▁live',\n",
       " '▁çıkartıldı',\n",
       " '▁standart',\n",
       " '▁duman',\n",
       " '▁açıklıkta',\n",
       " '▁yararl',\n",
       " '▁spears',\n",
       " '▁novorossiysk',\n",
       " '▁gösteri',\n",
       " '▁heykel',\n",
       " '▁gemiler',\n",
       " 'izm',\n",
       " 'q',\n",
       " '▁twitter',\n",
       " 'ibinde',\n",
       " '▁girişim',\n",
       " '▁restor',\n",
       " '▁iktidarı',\n",
       " '▁1994',\n",
       " 'son',\n",
       " '▁başkent',\n",
       " '▁yeniden',\n",
       " '▁yarış',\n",
       " 'damla',\n",
       " '▁kullanılan',\n",
       " '▁evler',\n",
       " '▁popülasyonu',\n",
       " '▁referandumu',\n",
       " 'buklar',\n",
       " '▁planı',\n",
       " '▁charlotte',\n",
       " 'html',\n",
       " 'fantasti',\n",
       " '▁1919',\n",
       " '▁altındaki',\n",
       " '▁muharebe',\n",
       " '▁çekimleri',\n",
       " '▁yönler',\n",
       " 'metri',\n",
       " '▁ad',\n",
       " '▁bunu',\n",
       " '▁dişi',\n",
       " 'asından',\n",
       " 'zonguldak',\n",
       " '▁dört',\n",
       " '▁amerikan',\n",
       " '▁196',\n",
       " '▁ediyor',\n",
       " 'ildi',\n",
       " '▁imzalan',\n",
       " '▁komuta',\n",
       " '▁3.',\n",
       " '▁tabak',\n",
       " '▁açıklan',\n",
       " '▁sebebiyle',\n",
       " 'esaret',\n",
       " 'lıyordu',\n",
       " 'dalga',\n",
       " 'ttifak',\n",
       " 'müştür',\n",
       " '▁geçmiş',\n",
       " '▁kullanarak',\n",
       " 'ة',\n",
       " '▁jennifer',\n",
       " '▁giden',\n",
       " '▁şube',\n",
       " '▁marka',\n",
       " '▁hukuk',\n",
       " '▁işgalin',\n",
       " '▁kralı',\n",
       " '▁general',\n",
       " '▁henderson',\n",
       " '▁1983',\n",
       " 'peterburg',\n",
       " '▁kalkan',\n",
       " '▁karsinojen',\n",
       " 'diye',\n",
       " '▁değerli',\n",
       " '▁eğ',\n",
       " 'ğ',\n",
       " '▁aktif',\n",
       " '▁rağmen',\n",
       " '▁olacaktı',\n",
       " '▁ulaşı',\n",
       " '▁derneği',\n",
       " '▁projeler',\n",
       " '▁arasında',\n",
       " '▁kelimesi',\n",
       " 'ы',\n",
       " '▁bolşevikler',\n",
       " 'lık',\n",
       " '▁edi',\n",
       " '▁düşün',\n",
       " '▁ajan',\n",
       " '▁hikâye',\n",
       " '▁sharr',\n",
       " '▁2012',\n",
       " '▁yürü',\n",
       " 'gi',\n",
       " '▁niha',\n",
       " '▁salonu',\n",
       " '▁hikâyesin',\n",
       " '▁töreni',\n",
       " 'bor',\n",
       " '▁son',\n",
       " 'ster',\n",
       " '▁istemektedir',\n",
       " '▁malzeme',\n",
       " 'lüğü',\n",
       " '▁20',\n",
       " 'sında',\n",
       " '▁ilk',\n",
       " '▁şaşırt',\n",
       " '▁hastanesi',\n",
       " '▁kızı',\n",
       " '▁amaçlı',\n",
       " '▁boer',\n",
       " 'düğünü',\n",
       " 'is',\n",
       " '▁feth',\n",
       " '▁destek',\n",
       " '▁büyü',\n",
       " '▁milyar',\n",
       " '▁hardcore',\n",
       " '▁süreç',\n",
       " '▁gemisi',\n",
       " '▁otur',\n",
       " 'ad',\n",
       " '▁katmanı',\n",
       " 'pano',\n",
       " 'kov',\n",
       " 'ntılı',\n",
       " '▁mekan',\n",
       " '▁yapısı',\n",
       " '▁anlamı',\n",
       " '▁ankastre',\n",
       " '▁çekilme',\n",
       " '▁hayvanları',\n",
       " '▁rick',\n",
       " 'ş',\n",
       " '▁yağmala',\n",
       " '▁kalacağın',\n",
       " '▁gözlemevi',\n",
       " 'bb',\n",
       " '▁ref',\n",
       " '▁şampiyonluğu',\n",
       " '▁1917',\n",
       " '▁sahte',\n",
       " 's',\n",
       " '▁yağ',\n",
       " '▁akşener',\n",
       " '▁kalkolitik',\n",
       " '▁mis',\n",
       " '▁tanıştı',\n",
       " '▁ağır',\n",
       " '▁aday',\n",
       " '▁ortam',\n",
       " '▁içerir',\n",
       " '▁1921',\n",
       " '▁mta',\n",
       " 'ankara',\n",
       " '▁kadınlar',\n",
       " '▁kıbrıs',\n",
       " '▁birey',\n",
       " '▁keşfedi',\n",
       " 'lerinde',\n",
       " '▁zombie',\n",
       " '▁başlarında',\n",
       " '▁etmiş',\n",
       " '▁çıkarır',\n",
       " '▁olur',\n",
       " '▁cs',\n",
       " '▁cezalandır',\n",
       " '▁teoloji',\n",
       " '▁itibaren',\n",
       " '▁sür',\n",
       " '▁açık',\n",
       " '▁charming',\n",
       " '▁ana',\n",
       " '▁jacob',\n",
       " '▁hood',\n",
       " '▁kokhba',\n",
       " '▁hedefi',\n",
       " '▁emri',\n",
       " '▁arz',\n",
       " '▁gösterilir',\n",
       " 'inden',\n",
       " '▁izleme',\n",
       " '▁parça',\n",
       " '▁kendisini',\n",
       " '▁piyasay',\n",
       " '▁çalışmış',\n",
       " '▁başvuranlar',\n",
       " '▁rakib',\n",
       " '▁boru',\n",
       " 'karsinojenler',\n",
       " '▁dağıl',\n",
       " '▁kolay',\n",
       " '▁başlaması',\n",
       " '▁bahçe',\n",
       " 'eklenen',\n",
       " '▁olmaya',\n",
       " '▁giren',\n",
       " '▁iyi',\n",
       " '▁üre',\n",
       " 'söz',\n",
       " '▁benzer',\n",
       " '▁meiji',\n",
       " '▁כוזיבא',\n",
       " 'kamış',\n",
       " '▁moskova',\n",
       " 'karsinojeni',\n",
       " '4',\n",
       " '▁höyükte',\n",
       " '▁kont',\n",
       " '▁1960',\n",
       " '▁sorumluluğu',\n",
       " '▁donanma',\n",
       " '▁belir',\n",
       " '▁tcg',\n",
       " 'ette',\n",
       " 'ânı',\n",
       " '▁pruva',\n",
       " '▁gri',\n",
       " 'з',\n",
       " '▁kaybet',\n",
       " '▁tasvir',\n",
       " 'ka',\n",
       " 'leşme',\n",
       " 'bilmek',\n",
       " '▁iddia',\n",
       " '▁edebiyat',\n",
       " 'yormuş',\n",
       " 'hü',\n",
       " '▁george',\n",
       " '▁hayran',\n",
       " '▁ürünlerin',\n",
       " '▁sağlayan',\n",
       " 'ric',\n",
       " '▁cevap',\n",
       " 'ham',\n",
       " '▁sayfalar',\n",
       " 'essi',\n",
       " '▁norfolk',\n",
       " '▁ifade',\n",
       " '▁fethedil',\n",
       " '▁ederken',\n",
       " '▁düşündüğü',\n",
       " 'bağlam',\n",
       " '▁alışveriş',\n",
       " '▁arasındaki',\n",
       " '▁sevgilisi',\n",
       " '▁kitaplar',\n",
       " '▁maggie',\n",
       " '▁arkadaşları',\n",
       " '▁atlanti',\n",
       " '▁ettikleri',\n",
       " '▁çakmak',\n",
       " '▁reznor',\n",
       " '▁azot',\n",
       " '▁savunduğu',\n",
       " '▁kariyerin',\n",
       " '▁gelen',\n",
       " '▁alınama',\n",
       " '▁çev',\n",
       " 'ge',\n",
       " '▁turbo',\n",
       " '▁kuvvetleri',\n",
       " '▁resmî',\n",
       " '▁ilçesi',\n",
       " '▁peribaca',\n",
       " '▁unvanı',\n",
       " 'niz',\n",
       " '▁profes',\n",
       " '▁ormanlarda',\n",
       " 'sis',\n",
       " '▁mysen',\n",
       " '▁kocası',\n",
       " '▁peg',\n",
       " '▁gereksinimler',\n",
       " 'מ',\n",
       " 'burgring',\n",
       " 'ret',\n",
       " '▁karayiplerde',\n",
       " '▁adıyla',\n",
       " '▁çeşit',\n",
       " '▁madde',\n",
       " '▁kuru',\n",
       " 'gat',\n",
       " '°',\n",
       " '▁helin',\n",
       " '▁onların',\n",
       " '▁dönmesin',\n",
       " '▁britanyalı',\n",
       " '▁devam',\n",
       " '▁1976',\n",
       " '▁bundesliga',\n",
       " 'türkiye',\n",
       " '▁yapılmış',\n",
       " 'imetre',\n",
       " '▁bolşevik',\n",
       " '▁geliştirdi',\n",
       " '▁sonra',\n",
       " 'ɪ',\n",
       " '▁olma',\n",
       " '▁etmeye',\n",
       " '▁otomobil',\n",
       " '▁istasyonu',\n",
       " '▁geçtiği',\n",
       " '▁|',\n",
       " 'dığı',\n",
       " '▁benz',\n",
       " '▁rakibi',\n",
       " '▁olay',\n",
       " '▁tarayıcı',\n",
       " 'zan',\n",
       " '▁tanrı',\n",
       " 'doğdu',\n",
       " '▁kural',\n",
       " '▁gönder',\n",
       " '▁koyun',\n",
       " '▁arkadaş',\n",
       " 'keitel',\n",
       " '▁yönel',\n",
       " '▁sevkiyat',\n",
       " 'w',\n",
       " '▁1971',\n",
       " '▁takımı',\n",
       " '▁hayvan',\n",
       " 'rlanda',\n",
       " '▁düştü',\n",
       " '▁uyan',\n",
       " '▁ilgi',\n",
       " '▁gerçekleş',\n",
       " '▁yak',\n",
       " '▁ilerledi',\n",
       " '▁hasan',\n",
       " '▁dergi',\n",
       " '▁oluşum',\n",
       " 'rip',\n",
       " '▁kapasitesin',\n",
       " 'fold',\n",
       " 'mahalli',\n",
       " '▁reis',\n",
       " '▁eserleri',\n",
       " '▁başrolü',\n",
       " '▁sch',\n",
       " '▁depp',\n",
       " '▁dretnot',\n",
       " 'م',\n",
       " '▁benze',\n",
       " '▁görünümünü',\n",
       " 'ulaktartış',\n",
       " '▁1980',\n",
       " '▁felsefesi',\n",
       " '▁esas',\n",
       " '▁pay',\n",
       " '▁eser',\n",
       " '▁alarak',\n",
       " '▁edilmesi',\n",
       " 'ə',\n",
       " '▁çağlar',\n",
       " '▁evliliği',\n",
       " '▁gösteriler',\n",
       " '▁dağı',\n",
       " '▁başar',\n",
       " 'mek',\n",
       " '▁kadar',\n",
       " '▁üst',\n",
       " '▁altınordu',\n",
       " '▁görev',\n",
       " '▁deney',\n",
       " '▁musevi',\n",
       " '▁yapım',\n",
       " '▁bölgelerde',\n",
       " '▁fırlattı',\n",
       " '▁13',\n",
       " '▁universit',\n",
       " '▁değerlendir',\n",
       " '▁neden',\n",
       " '▁kitabı',\n",
       " 'larda',\n",
       " 'montreal',\n",
       " '▁şakıma',\n",
       " 'otokrasi',\n",
       " 'uncu',\n",
       " '25',\n",
       " '▁aşağıdaki',\n",
       " 'meci',\n",
       " 'لا',\n",
       " 'ıyla',\n",
       " '▁vasıtası',\n",
       " '▁david',\n",
       " 'ücü',\n",
       " '▁yayınlanmış',\n",
       " '▁futbolcu',\n",
       " 'şleri',\n",
       " '▁bünyesin',\n",
       " '▁doğrultu',\n",
       " '▁kurulan',\n",
       " '▁esnek',\n",
       " '▁derin',\n",
       " '▁mehmed',\n",
       " '▁simon',\n",
       " '▁merkez',\n",
       " '▁aç',\n",
       " '▁uzay',\n",
       " '▁2006',\n",
       " '▁dizide',\n",
       " '▁toplum',\n",
       " '▁figür',\n",
       " '▁engelleme',\n",
       " '▁sesler',\n",
       " 'asyon',\n",
       " '▁35',\n",
       " 'büyükelçi',\n",
       " '▁karakter',\n",
       " '▁sektör',\n",
       " 'th',\n",
       " '▁tren',\n",
       " '▁yerel',\n",
       " '▁ayır',\n",
       " 'kimliği',\n",
       " '▁çoğaltma',\n",
       " '▁hazır',\n",
       " 'dığında',\n",
       " '▁ayrılı',\n",
       " 'ع',\n",
       " '▁güc',\n",
       " '▁ederek',\n",
       " '▁yıl',\n",
       " 'aş',\n",
       " '▁ahme',\n",
       " '▁kalır',\n",
       " 'lanmış',\n",
       " 'lendirdi',\n",
       " '▁serge',\n",
       " '▁bulunmuş',\n",
       " '▁arttır',\n",
       " '▁gansel',\n",
       " '▁bayer',\n",
       " '▁oğlak',\n",
       " 'larını',\n",
       " '▁ağırlığındadır',\n",
       " '▁evlen',\n",
       " 'tılar',\n",
       " 'charlie',\n",
       " '▁adaları',\n",
       " '▁okuduğu',\n",
       " '▁yumurta',\n",
       " '▁gözlemlenm',\n",
       " '▁robert',\n",
       " '▁otlatma',\n",
       " '▁sayılı',\n",
       " 'ש',\n",
       " '▁mal',\n",
       " '▁kül',\n",
       " '&',\n",
       " 'ılmasını',\n",
       " '▁yelpazesi',\n",
       " 'و',\n",
       " '▁şirketlerin',\n",
       " 'ra',\n",
       " '▁ihtimal',\n",
       " '▁burslu',\n",
       " 'iihf',\n",
       " '▁çıkmış',\n",
       " '▁yaşamını',\n",
       " '▁sürdürüle',\n",
       " '▁benimsenme',\n",
       " '▁şehrinde',\n",
       " '▁kaynak',\n",
       " '▁altı',\n",
       " '▁araya',\n",
       " '▁sayısında',\n",
       " '▁seç',\n",
       " '▁26',\n",
       " '▁dizgi',\n",
       " '▁sunmaktaydı',\n",
       " 'kin',\n",
       " '▁fa',\n",
       " 'tı',\n",
       " '▁sağ',\n",
       " 'anıtı',\n",
       " '▁kaçırma',\n",
       " '▁daniel',\n",
       " '“',\n",
       " 'ns',\n",
       " '▁kemik',\n",
       " 'senaryo',\n",
       " '▁66',\n",
       " '▁yazar',\n",
       " 'ırlar',\n",
       " '▁aziz',\n",
       " '▁çağır',\n",
       " 'ndirme',\n",
       " 'stalin',\n",
       " '▁başbakan',\n",
       " '▁bucaspor',\n",
       " '▁will',\n",
       " '▁kuruldu',\n",
       " '▁esnasında',\n",
       " '▁oğlağı',\n",
       " '▁sayede',\n",
       " '▁bilinen',\n",
       " '▁eagle',\n",
       " '▁düşürmüştü',\n",
       " 'port',\n",
       " 'uşturdu',\n",
       " '▁becker',\n",
       " '▁gün',\n",
       " '▁şehirde',\n",
       " '▁istedi',\n",
       " '▁problemi',\n",
       " 'س',\n",
       " '▁mülkiyet',\n",
       " '▁bogovinska',\n",
       " 'λ',\n",
       " 'span',\n",
       " '▁britanya',\n",
       " '▁anadolu',\n",
       " '▁fransız',\n",
       " '▁uzlaşma',\n",
       " '▁pres',\n",
       " 'lev',\n",
       " '▁bölgenin',\n",
       " '▁anabilim',\n",
       " '▁biridir',\n",
       " 'mud',\n",
       " '▁çözüm',\n",
       " '▁kadın',\n",
       " '▁mesafe',\n",
       " '▁süre',\n",
       " '▁alı',\n",
       " 'muşlardır',\n",
       " '▁sözcüğü',\n",
       " 'mişlerdir',\n",
       " 'ça',\n",
       " '▁paris',\n",
       " '▁boşanma',\n",
       " '▁ayrıca',\n",
       " '▁tam',\n",
       " '1912',\n",
       " '▁benimsenmiş',\n",
       " '▁çeken',\n",
       " 'direkt',\n",
       " '100',\n",
       " '▁sev',\n",
       " '▁yarısı',\n",
       " 'şa',\n",
       " \".00'\",\n",
       " 'lk',\n",
       " '▁tasar',\n",
       " '▁gelmesi',\n",
       " '▁anap',\n",
       " '▁mo',\n",
       " '▁pizza',\n",
       " '▁yoktur',\n",
       " '▁yapıldığı',\n",
       " '▁lee',\n",
       " '▁ilerley',\n",
       " '▁pap',\n",
       " '▁hatırla',\n",
       " '▁atan',\n",
       " '▁konuların',\n",
       " 'zmit',\n",
       " '▁ikisini',\n",
       " '▁batı',\n",
       " '▁yunan',\n",
       " 'plak',\n",
       " '▁fiziksel',\n",
       " '▁balboa',\n",
       " '▁kanadalı',\n",
       " '▁şiddetl',\n",
       " 'ian',\n",
       " 'öğüt',\n",
       " '▁çekişme',\n",
       " 'olog',\n",
       " 'leştirerek',\n",
       " '▁fransa',\n",
       " '▁dünyası',\n",
       " '▁yarışlarında',\n",
       " 'eric',\n",
       " '▁serbest',\n",
       " 'solo',\n",
       " 'larından',\n",
       " '▁mail',\n",
       " '▁hunter',\n",
       " '▁dini',\n",
       " '▁benimseyen',\n",
       " '▁çin',\n",
       " '▁rakip',\n",
       " '▁kraliçe',\n",
       " '▁yüksel',\n",
       " '▁daryl',\n",
       " '▁canlı',\n",
       " '▁yangını',\n",
       " '▁fighter',\n",
       " '▁kahverengi',\n",
       " 'lerini',\n",
       " '▁çıkıp',\n",
       " '▁ban',\n",
       " '▁anlatım',\n",
       " '▁vazgeç',\n",
       " '▁savaşı',\n",
       " '▁marmara',\n",
       " '▁inşa',\n",
       " '▁ülkeleri',\n",
       " '▁sor',\n",
       " 'leri',\n",
       " 'tip',\n",
       " '▁renkleri',\n",
       " '▁düşünü',\n",
       " '▁din',\n",
       " 'í',\n",
       " '▁tersane',\n",
       " '▁tanıttı',\n",
       " '▁geminin',\n",
       " '37',\n",
       " 'le',\n",
       " '▁lider',\n",
       " '▁soğuk',\n",
       " '▁hatta',\n",
       " '▁kimsenin',\n",
       " '▁70',\n",
       " 'tion',\n",
       " 'abileceğini',\n",
       " '▁müdahale',\n",
       " '▁kullanan',\n",
       " 'çu',\n",
       " '▁devletler',\n",
       " '▁dio',\n",
       " 'nabilir',\n",
       " '▁devlet',\n",
       " '▁fen',\n",
       " 'baş',\n",
       " '▁buz',\n",
       " '▁taş',\n",
       " '▁geçiril',\n",
       " '▁konsantrasyon',\n",
       " 'ұ',\n",
       " 'ımı',\n",
       " 'ileceğin',\n",
       " '▁bombala',\n",
       " 'way',\n",
       " '▁seçil',\n",
       " '▁boks',\n",
       " '▁eklen',\n",
       " '11',\n",
       " '▁sok',\n",
       " '▁günü',\n",
       " '▁cümle',\n",
       " '▁banabi',\n",
       " '▁ozzfest',\n",
       " 'diğini',\n",
       " '▁trafiği',\n",
       " '▁menemen',\n",
       " 'klarını',\n",
       " 'lerden',\n",
       " 'batısında',\n",
       " '▁petrol',\n",
       " '▁100',\n",
       " '▁tekniğin',\n",
       " '▁görüntü',\n",
       " 'jür',\n",
       " '▁muranov',\n",
       " '▁kardeşi',\n",
       " '▁harekât',\n",
       " '▁step',\n",
       " '▁itira',\n",
       " '▁köyleri',\n",
       " '▁kraliyet',\n",
       " 'leriyle',\n",
       " 'yı',\n",
       " '▁yapısında',\n",
       " '▁de',\n",
       " '▁kurallar',\n",
       " '▁olup',\n",
       " '▁sol',\n",
       " '▁güreş',\n",
       " '▁girer',\n",
       " '▁askerleri',\n",
       " '▁geri',\n",
       " '▁armutlu',\n",
       " '▁göç',\n",
       " '▁kızıl',\n",
       " '49',\n",
       " '▁angels',\n",
       " '▁anısına',\n",
       " 'ayet',\n",
       " '▁faaliyetlerin',\n",
       " '▁toprakların',\n",
       " '▁kaybettiği',\n",
       " '▁korkuluklar',\n",
       " 'dikleri',\n",
       " '▁sonuç',\n",
       " '▁açıl',\n",
       " '▁ermeniler',\n",
       " '▁üstünde',\n",
       " '▁haber',\n",
       " '▁oluşturma',\n",
       " '▁platform',\n",
       " '▁oluşturulmuş',\n",
       " '▁odacıklar',\n",
       " '▁sözleri',\n",
       " '▁durumun',\n",
       " '▁komünist',\n",
       " '▁yatağın',\n",
       " '▁modeli',\n",
       " 'ahi',\n",
       " '▁nokta',\n",
       " '▁metre',\n",
       " '▁sezonu',\n",
       " 'hoş',\n",
       " '▁ölümünden',\n",
       " '▁gerçek',\n",
       " 'ἰ',\n",
       " '▁ryu',\n",
       " 'bakanlar',\n",
       " '▁değişme',\n",
       " 'entertainment',\n",
       " '▁şirketi',\n",
       " 'å',\n",
       " '▁mar',\n",
       " 'tar',\n",
       " '▁köy',\n",
       " '▁isyan',\n",
       " '▁kökpar',\n",
       " '▁oyunları',\n",
       " '▁törende',\n",
       " '▁isteğin',\n",
       " '▁sağlamak',\n",
       " '▁duygus',\n",
       " '▁194',\n",
       " '▁300',\n",
       " '▁har',\n",
       " '▁erken',\n",
       " '▁kamp',\n",
       " '▁yakınında',\n",
       " '▁richard',\n",
       " '▁gösterdiği',\n",
       " '▁ulaşmıştır',\n",
       " '▁kuşlar',\n",
       " '▁savunmasız',\n",
       " '▁sezgi',\n",
       " 'zim',\n",
       " '▁militari',\n",
       " '▁kara',\n",
       " 'olonya',\n",
       " 'melos',\n",
       " '▁işlevlerin',\n",
       " '▁kalan',\n",
       " '.000',\n",
       " '▁mecra',\n",
       " '▁keşi',\n",
       " '▁höyüğü',\n",
       " 'iam',\n",
       " 'key',\n",
       " 'leşmiş',\n",
       " '▁seçim',\n",
       " 'işe',\n",
       " '▁etkilen',\n",
       " '▁21',\n",
       " '▁söylenir',\n",
       " '▁yürütül',\n",
       " '▁cephane',\n",
       " '▁gerekli',\n",
       " '▁toprak',\n",
       " '▁jord',\n",
       " '▁içer',\n",
       " '▁yani',\n",
       " '▁vekiller',\n",
       " '▁kelimelerin',\n",
       " 'қ',\n",
       " '▁insanlar',\n",
       " 'eytin',\n",
       " '▁yaşar',\n",
       " 'ten',\n",
       " 'düşer',\n",
       " '▁1889',\n",
       " 'iyeti',\n",
       " '▁enkaz',\n",
       " '▁zaman',\n",
       " '▁depo',\n",
       " '▁başlattı',\n",
       " '▁irade',\n",
       " '▁kars',\n",
       " '▁yaygın',\n",
       " '▁cenevre',\n",
       " 'hang',\n",
       " '▁sistemleri',\n",
       " '▁bağlanmış',\n",
       " 'm',\n",
       " ...}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"▁salyangozlar\" in wiki_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"türkiye\" in wiki_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"futbol\" in wiki_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4961"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.stoi[\"türkiye\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Fake Wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = \"Bilim ve Teknik türkiye\"\n",
    "# TEXT = \"Temmuz ayı içerisinde\"\n",
    "N_WORDS = 40\n",
    "N_SENTENCES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temmuz ayı içerisinde mak mayı da ▁( ▁w ▁xxmaj . i ὁ ▁gök ▁maçın ▁rağmen ▁xxmaj ▁konusu ▁xxmaj in y ▁listesi h z ’ lik ▁yapılan ) 190 u . un ▁xxmaj , e ▁ile ▁2001 ▁xxmaj ▁ ▁ ▁xxmaj ▁i - ,\n",
      "Temmuz ayı içerisinde , ▁ . . ' ▁mitolojisinde ▁xxmaj ▁ ya ▁xxmaj ▁miuccia klarını ▁ ▁yıllarda ' , ̇ ▁xxmaj , dü ▁xxup ▁xxmaj ▁xxmaj , ▁( i ▁bu ya ▁\" ▁\" mesi ün ▁xxmaj ▁xxmaj ▁veril ▁xxmaj ▁( ' - se\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bilim ve Teknik türkiye ro st ▁xxmaj ▁xxmaj tı ▁xxmaj ▁xxmaj . ▁xxmaj daha n lar ▁xxmaj , a . ▁bıraktı ▁xxmaj . ' . ▁xxmaj en nda ▁vardır dı ▁kapak , ▁yüksekokulu e ▁ yı ▁xxup ▁futbolcu di , ün ın ▁ ▁xxmaj\n",
      "Bilim ve Teknik türkiye ▁ter ▁ve ▁xxmaj ▁ . ▁ve i ▁xxup ▁ at g le si up ▁ ▁xxmaj ▁ve ▁xxmaj ▁xxmaj ▁civar ▁ve m ▁ ▁seçil ▁sonra ▁xxmaj ▁tarihinde ▁uzun ▁xxmaj ▁xxmaj ▁bir ▁xxmaj ▁xxmaj ▁xxmaj ▁bir ▁c ▁farklı ▁diğer ▁xxmaj ▁xxmaj\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.80) for _ in range(N_SENTENCES)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turkish sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.win.tue.nl/~mpechen/projects/smm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_clas = path/'movies'\n",
    "path_clas.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = (path_clas/'tr_polarity.pos').open(encoding='iso-8859-9').readlines()\n",
    "pos_df = pd.DataFrame({'text':pos})\n",
    "pos_df['pos'] = 1\n",
    "pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = (path_clas/'tr_polarity.neg').open(encoding='iso-8859-9').readlines()\n",
    "neg_df = pd.DataFrame({'text':neg})\n",
    "neg_df['pos'] = 0\n",
    "neg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pos_df,neg_df], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = (TextList.from_df(df, path_clas, cols='text', processor=SPProcessor.load(dest))\n",
    "    .split_by_rand_pct(0.1, seed=42)\n",
    "    .label_for_lm()           \n",
    "    .databunch(bs=bs, num_workers=1))\n",
    "\n",
    "data_lm.save(f'{lang}_clas_databunch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = load_data(path_clas, f'{lang}_clas_databunch', bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm = language_model_learner(data_lm, AWD_LSTM, pretrained_fnames=lm_fns, drop_mult=1.0, wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "lr *= bs/48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm.fit_one_cycle(1, lr*10, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm.unfreeze()\n",
    "learn_lm.fit_one_cycle(5, slice(lr/10,lr*10), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm.save(f'{lang}fine_tuned')\n",
    "learn_lm.save_encoder(f'{lang}fine_tuned_enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = (TextList.from_df(df, path_clas, cols='text', processor=SPProcessor.load(dest))\n",
    "    .split_by_rand_pct(0.1, seed=42)\n",
    "    .label_from_df(cols='pos')\n",
    "    .databunch(bs=bs, num_workers=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_c = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5, pretrained=False, wd=0.1).to_fp16()\n",
    "learn_c.load_encoder(f'{lang}fine_tuned_enc')\n",
    "learn_c.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=2e-2\n",
    "lr *= bs/48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn_c.fit_one_cycle(2, lr, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn_c.fit_one_cycle(2, lr, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_c.freeze_to(-2)\n",
    "learn_c.fit_one_cycle(2, slice(lr/(2.6**4),lr), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_c.freeze_to(-3)\n",
    "learn_c.fit_one_cycle(2, slice(lr/2/(2.6**4),lr/2), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_c.unfreeze()\n",
    "learn_c.fit_one_cycle(4, slice(lr/10/(2.6**4),lr/10), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy in Gezici (2018), *Sentiment Analysis in Turkish* is: `75.16%`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_c.save(f'{lang}clas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
