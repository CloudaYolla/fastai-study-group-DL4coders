{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HBA PA4\n",
    "\n",
    "French version: https://github.com/Daniel-R-Armstrong/French-Wiki-2500-Pretrained-SentencePiece-LM/blob/master/fr_spm.ipynb\n",
    "https://forums.fast.ai/t/sentencepiece/53010/50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turkish ULMFiT from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #hide\n",
    "# !pip install -Uqq fastbook\n",
    "# import fastbook\n",
    "# fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install fastai==1.0.61 #This right here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install fastcore==1. #This right here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #fastai v1 API: https://fastai1.fast.ai/overview.html\n",
    "# from fastai.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai.utils import *\n",
    "# show_install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from fastai import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print ('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('/home/ec2-user/.fastai/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=128\n",
    "torch.cuda.set_device(2)\n",
    "# data_path = Config.data_path()\n",
    "# data_path = '/home/ec2-user/.fastai/data'\n",
    "\n",
    "lang = 'tx'\n",
    "name = f'{lang}wiki'\n",
    "path = data_path/name\n",
    "path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_path = path/'models'\n",
    "mdl_path.mkdir(exist_ok=True)\n",
    "lm_fns = [mdl_path/f'{lang}_wt', mdl_path/f'{lang}_wt_vocab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ec2-user/.fastai/data/txwiki')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tx'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turkish wikipedia model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #HBA\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install wikiextractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wikiextractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlputils import split_wiki,get_wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncompress wiki file from cli\n",
    "\n",
    "Source: https://github.com/attardi/wikiextractor\n",
    "\n",
    "1. Do `python setup.py install`\n",
    "1. Double check with: `which wikiextractor`\n",
    "\n",
    "`\n",
    "base) [ec2-user@ip-172-16-69-8 trwiki]$ which wikiextractor\n",
    "~/anaconda3/bin/wikiextractor\n",
    "`\n",
    "\n",
    "1. Uncrompress\n",
    "\n",
    "`\n",
    "(base) [ec2-user@ip-172-16-69-8 trwiki]$ python -m wikiextractor.WikiExtractor trwiki-latest-pages-articles.xml\n",
    "INFO: 2832129   Mucize Dünyası: New York – Birleşik Kahramanlar\n",
    "INFO: 2832111   Kraliyet Mühendislik Akademisi üyesi kadınlar listesi\n",
    "INFO: 2832131   Ilana Rovina\n",
    "INFO: 2832155   Artur Taymazov\n",
    "INFO: 2832130   Enka (kadın voleybol takımı) 1992-93 sezonu\n",
    "INFO: Finished 31-process extraction of 377951 articles in 123.1s (3071.5 art/s)\n",
    "INFO: total of page: 732729, total of articl page: 377951; total of used articl page: 377951\n",
    "(base) [ec2-user@ip-172-16-69-8 trwiki]$ which wikiextractor\n",
    "~/anaconda3/bin/wikiextractor\n",
    "`\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading...\n",
      "unzipping...\n",
      "extracting... akirmak\n"
     ]
    }
   ],
   "source": [
    "get_wiki(path,lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<doc id=\"10\" url=\"https://tr.wikipedia.org/wiki?curid=10\" title=\"Cengiz Han\">\r\n",
      "Cengiz Han\r\n",
      "\r\n",
      "Cengiz Han (doğum Temuçin, – 18 Ağustos 1227) ölümünden sonra tarihin en büyük yüzölçümüne sahip imparatorluğu haline gelmiş Moğol İmparatorluğu'nun kurucusu ve ilk Kağanı ve İmparatoru olan Moğol komutan ve hükümdar. 13. Yüzyılın başında Orta Asya'daki tüm göçebe bozkır kavimlerini birleştirerek bir ulus hâline getirdi ve o ulusu \"Moğol\" siyasi kimliği çatısı altında topladı.\r\n"
     ]
    }
   ],
   "source": [
    "# from nlputils import split_wiki,get_wiki\n",
    "\n",
    "# get_wiki(path,lang)\n",
    "!head -n4 {path}/{name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/ec2-user/.fastai/data/txwiki/wikiextractor-orig'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/txwiki-latest-pages-articles.xml'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/txwiki-latest-pages-articles.xml.bz2'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/log'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/wikiextractor'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/docs'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/models'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/txwiki')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.fastai/data/txwiki/docs already exists; not splitting\n"
     ]
    }
   ],
   "source": [
    "dest = split_wiki(path,lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/ec2-user/.fastai/data/txwiki/docs/Kara delik.txt'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/docs/Heaviside katmanı.txt'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/docs/Zurna.txt'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/docs/Özallı Yıllar.txt'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/docs/Kuşça, Cihanbeyli.txt')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest.ls()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ec2-user/.fastai/data/txwiki/docs')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turkish is an [Agglutinative_language](https://en.wikipedia.org/wiki/Agglutinative_language) so it needs special care!\n",
    "\n",
    "![Turkish morphemes example](images/turkish.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HBA fix\n",
    "\n",
    "1. Open via: `vi ~/SageMaker/.env/fastai/lib/python3.6/site-packages/fastai/text/data.py`\n",
    "1. see: https://forums.fast.ai/t/multifit-runtime-error-permission-denied/72874/5\n",
    "1. See: https://github.com/fastai/fastai/issues/2572 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ec2-user/.fastai/data/txwiki/docs')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/ec2-user/.fastai/data/txwiki/docs/Kara delik.txt'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/docs/Heaviside katmanı.txt'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/docs/Zurna.txt'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/docs/Özallı Yıllar.txt'),\n",
       " PosixPath('/home/ec2-user/.fastai/data/txwiki/docs/Kuşça, Cihanbeyli.txt')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest.ls()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56219\r\n"
     ]
    }
   ],
   "source": [
    "#PRINT NUMBER OF FILES IN DIR\n",
    "!find /home/ec2-user/.fastai/data/txwiki/docs -mindepth 1 -maxdepth 1 -printf '.' | wc -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE ALL BUT 1O FILES\n",
    "# !ls -1tr /home/ec2-user/.fastai/data/txwiki/docs | head -n -100 | xargs -d '\\n' rm -f --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # x.sh\n",
    "# cp \"Zurna.txt\" ../docs-100/\n",
    "# cp \"Ulucak Höyüğü.txt\" ../docs-100/\n",
    "# cp \"Özallı Yıllar.txt\" ../docs-100/\n",
    "# cp \"Kuşça, Cihanbeyli.txt\" ../docs-100/\n",
    "# cp \"Kara delik.txt\" ../docs-100/\n",
    "# cp \"Heaviside katmanı.txt\" ../docs-100/\n",
    "# cp \"Çağlar Söyüncü.txt\" ../docs-100/\n",
    "# cp \"Bilim ve Teknik.txt\" ../docs-100/\n",
    "# cp \"Arş.txt\" ../docs-100/\n",
    "# cp \"Amnesia: Dark Descent.txt\" ../docs-100/\n",
    "# cp \"Yunancanın romanizasyonu.txt\" ../docs-100/\n",
    "# cp \"Yön Tanrıları.txt\" ../docs-100/\n",
    "# cp \"Yemeksepeti.com.txt\" ../docs-100/\n",
    "# cp \"Yaman, Mudanya.txt\" ../docs-100/\n",
    "# cp \"WWE 24_7 Championship.txt\" ../docs-100/\n",
    "# cp \"Wilhelm Reich.txt\" ../docs-100/\n",
    "# cp \"Web grafik tasarımı.txt\" ../docs-100/\n",
    "# cp \"Vuslat Doğan Sabancı.txt\" ../docs-100/\n",
    "# cp \"Vladislav Tretyak.txt\" ../docs-100/\n",
    "# cp \"VIII. Henry.txt\" ../docs-100/\n",
    "# cp \"Us (The Walking Dead).txt\" ../docs-100/\n",
    "# cp \"USS Arizona (BB-39).txt\" ../docs-100/\n",
    "# cp \"Türk Kurtuluş Savaşı sırasında katliamlar.txt\" ../docs-100/\n",
    "# cp \"TRT 1.txt\" ../docs-100/\n",
    "# cp \"Transvaal Cumhuriyeti.txt\" ../docs-100/\n",
    "# cp \"Trabzon Konferansı.txt\" ../docs-100/\n",
    "# cp \"Today Was a Fairytale.txt\" ../docs-100/\n",
    "# cp \"Tehlikeli Oyun (film, 2008).txt\" ../docs-100/\n",
    "# cp \"TCG Berk.txt\" ../docs-100/\n",
    "# cp \"Taziz.txt\" ../docs-100/\n",
    "# cp \"Taye Taiwo.txt\" ../docs-100/\n",
    "# cp \"Tatyana Volosojar.txt\" ../docs-100/\n",
    "# cp \"Tanzanya arması.txt\" ../docs-100/\n",
    "# cp \"Süleyman Türkünal.txt\" ../docs-100/\n",
    "# cp \"Street Fighter (video oyunu).txt\" ../docs-100/\n",
    "# cp \"Sons of Anarchy.txt\" ../docs-100/\n",
    "# cp \"Sol SR.txt\" ../docs-100/\n",
    "# cp \"Sloven İstihbarat ve Güvenlik Ajansı.txt\" ../docs-100/\n",
    "# cp \"Simon bar Kohba.txt\" ../docs-100/\n",
    "# cp \"Silverline Ankastre.txt\" ../docs-100/\n",
    "# cp \"Sıçansılar (balık).txt\" ../docs-100/\n",
    "# cp \"Shim Changmin.txt\" ../docs-100/\n",
    "# cp \"Seş.txt\" ../docs-100/\n",
    "# cp \"Şar Dağları.txt\" ../docs-100/\n",
    "# cp \"San Marino millî futbol takımı.txt\" ../docs-100/\n",
    "# cp \"Saga Norén.txt\" ../docs-100/\n",
    "# cp \"Robot of Sherwood.txt\" ../docs-100/\n",
    "# cp \"Refik Turan.txt\" ../docs-100/\n",
    "# cp \"RaceRoom.txt\" ../docs-100/\n",
    "# cp \"Porsche 911 GT2.txt\" ../docs-100/\n",
    "# cp \"Piçan İlçesi.txt\" ../docs-100/\n",
    "# cp \"Perşa Liha.txt\" ../docs-100/\n",
    "# cp \"Perpa.txt\" ../docs-100/\n",
    "# cp \"Pepsi Twist.txt\" ../docs-100/\n",
    "# cp \"Outlook Express.txt\" ../docs-100/\n",
    "# cp \"Öter ardıç kuşu.txt\" ../docs-100/\n",
    "# cp \"Østfold.txt\" ../docs-100/\n",
    "# cp \"Ortak varlıkların trajedisi.txt\" ../docs-100/\n",
    "# cp \"Ondokuz Mayıs Üniversitesi.txt\" ../docs-100/\n",
    "# cp \"Ömer Dinçer.txt\" ../docs-100/\n",
    "# cp \"Nissim Kamondo.txt\" ../docs-100/\n",
    "# cp \"Nif geni.txt\" ../docs-100/\n",
    "# cp \"Nick Hornby.txt\" ../docs-100/\n",
    "# cp \"Neo Geo.txt\" ../docs-100/\n",
    "# cp \"NBA Yılın En İyi Beşi.txt\" ../docs-100/\n",
    "# cp \"Nancy Atakan.txt\" ../docs-100/\n",
    "# cp \"Nacho Vidal.txt\" ../docs-100/\n",
    "# cp \"Mukor.txt\" ../docs-100/\n",
    "# cp \"Miuccia Prada.txt\" ../docs-100/\n",
    "# cp \"Millî marş.txt\" ../docs-100/\n",
    "# cp \"Millet İttifakı.txt\" ../docs-100/\n",
    "# cp \"Mikio Oda.txt\" ../docs-100/\n",
    "# cp \"Matvey Muranov.txt\" ../docs-100/\n",
    "# cp \"Maliye Bakanlığı (Azerbaycan).txt\" ../docs-100/\n",
    "# cp \"Malabadi Köprüsü.txt\" ../docs-100/\n",
    "# cp \"Makas Eller.txt\" ../docs-100/\n",
    "# cp \"Lund Üniversitesi.txt\" ../docs-100/\n",
    "# cp \"Liemba.txt\" ../docs-100/\n",
    "# cp \"LA X.txt\" ../docs-100/\n",
    "# cp \"Lando Buzzanca.txt\" ../docs-100/\n",
    "# cp \"Kutsal Duman.txt\" ../docs-100/\n",
    "# cp \"Kurtlar Vadisi bölümleri listesi.txt\" ../docs-100/\n",
    "# cp \"Kuch Kuch Hota Hai.txt\" ../docs-100/\n",
    "# cp \"Koyulhisar.txt\" ../docs-100/\n",
    "# cp \"Kökpar.txt\" ../docs-100/\n",
    "# cp \"Kitap tasarımı.txt\" ../docs-100/\n",
    "# cp \"Kelime.txt\" ../docs-100/\n",
    "# cp \"Kazatomprom.txt\" ../docs-100/\n",
    "# cp \"Kayseri'de 2009 Türkiye yerel seçimleri.txt\" ../docs-100/\n",
    "# cp \"Karpal tünel sendromu.txt\" ../docs-100/\n",
    "# cp \"Kanserojen.txt\" ../docs-100/\n",
    "# cp \"Joseph Henry.txt\" ../docs-100/\n",
    "# cp \"José Bosingwa.txt\" ../docs-100/\n",
    "# cp \"John 5.txt\" ../docs-100/\n",
    "# cp \"Je suis Charlie.txt\" ../docs-100/\n",
    "# cp \"Jesse Ventura.txt\" ../docs-100/\n",
    "# cp \"Jérôme Rothen.txt\" ../docs-100/\n",
    "# cp \"Japon militarizmi.txt\" ../docs-100/\n",
    "# cp \"Iustinus.txt\" ../docs-100/\n",
    "# cp \"İstihbarat Bakanlığı (İran).txt\" ../docs-100/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\r\n"
     ]
    }
   ],
   "source": [
    "#PRINT NUMBER OF FILES IN DIR\n",
    "!find /home/ec2-user/.fastai/data/txwiki/docs -mindepth 1 -maxdepth 1 -printf '.' | wc -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/fastai/text/data.py:476: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  ds.items = np.array(sum(e.map(self._encode_batch, partition_by_cores(ds.items, self.n_cpus)), []))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(5736, 90)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = (TextList.from_folder(dest, processor=[OpenFileProcessor(), SPProcessor()])\n",
    "        .split_by_rand_pct(0.1, seed=42)\n",
    "        .label_for_lm()\n",
    "        .databunch(bs=bs, num_workers=1))\n",
    "\n",
    "data.save(f'{lang}_databunch')\n",
    "len(data.vocab.itos),len(data.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(dest, f'{lang}_databunch', bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/fastai/text/data.py:339: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  idx_min = (t != self.pad_idx).nonzero().min()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>. ▁yükseklik ten ▁sonra ▁başlar . ▁xxmaj ▁bu ▁ismi n ▁veril mesi nin ▁sebebi ▁i yon osfer ▁bölgesinde ▁güneş ▁rüzg ar ı ▁ve ▁ko z mik ▁ ışık ▁gibi ▁uzay ▁kaynak lı ▁etkiler den ▁ötürü ▁i yon ▁yoğunluğu nun ▁yüksek ▁oluş u dur . ▁xxmaj ▁bu ▁i yon lar ▁çeşitli ▁ara ▁katman lara ▁yol ▁açar . ▁xxmaj ▁bilim ▁dünya sında ▁bu ▁ara ▁katman lar ▁xxup ▁ d , e ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>▁ar x e oloji ▁kazı n tılar ▁zaman ı ▁mar al ▁ boynuz un dan ▁hazırlanmış ▁dört ▁adet ▁zurna ▁ aş kar ▁edilmişti r . ▁xxmaj ▁bilim ▁adam larının ▁hesap lama larına ▁göre , ▁yüksek ▁ ze v k le ▁yapılmış ▁bu ▁çalgı ların ▁üç ▁bin ▁yıl ▁yaş ı ▁vardır . ▁xxmaj ▁esasen ▁erik , ▁ceviz , ▁ ya ba nı ▁ s öğüt ▁ve ▁ du t ▁ağa c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>▁xxmaj ▁demir el ' in ▁tekrar ▁başbakan ▁olması , ▁xxmaj ▁özal ' ın ▁hem ▁xxmaj ▁demir el ▁hem ▁de ▁xxmaj ▁yılmaz ▁ile ▁anlaşma z lığı , ▁xxmaj ▁özal ' ın ▁anap ' tan ▁ko p u şu , ▁yeni ▁parti ▁ arayışlar ı ▁ve ▁ ölümüne ▁yer ▁verildi . ▁&lt; ▁/ ▁ doc &gt; ▁xxbos ▁xxmaj ▁kuşça , ▁xxmaj ▁cihanbeyli ▁xxmaj ▁kuşca ▁( k ür t çe : ▁xxmaj ▁he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7 ▁cm ) ö l ç ü lerinde ▁basıl maya ▁başlan dı . ▁2001 ▁yılında ▁xxmaj ▁i ̇ r f an ▁xxmaj ▁say ar ' ın ▁çiz diği ▁xxmaj ▁ zi h ni ▁xxmaj ▁sinir ' in ▁\" p ro ce lerini \" ▁yayınla maya ▁başladı . ▁xxmaj ▁bu ▁yıl da ▁400 . ▁sayısında ▁( mar t ▁2001 ) ▁birlikte ▁ek ▁olarak ▁\" i ̇ nsan ▁xxmaj ▁genomu \" ▁verildi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>▁sonra ▁xxmaj ▁daniel ▁bir ▁ ışık ▁görür ▁ve ▁xxmaj ▁ag rip pa ' nın ▁sesi ▁duyul ur ; ▁xxmaj ▁w e ye r ' a ▁xxmaj ▁daniel ' ı ▁kurtar mak ▁için ▁bir ▁şey ▁yap masını ▁söyler ▁ve ▁xxmaj ▁daniel ▁hayat a ▁geri ▁döner . ▁&lt; br &gt; ▁xxmaj ▁gizli ▁xxmaj ▁son : ▁xxmaj ▁alexander , ▁xxmaj ▁daniel ' ı ▁hücre ye ▁atar ▁ve ▁ona ▁bekle mesi ni ▁söyler .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn = language_model_learner(data, AWD_LSTM, drop_mult=0.1, wd=0.1, pretrained=False).to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HBA French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='4' class='' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      25.00% [4/16 00:02<00:08 6.6273]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "learn.lr_find(stop_div=False, num_it=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to compute the gradients, there might not be enough points.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-3\n",
    "lr *= bs/48  # Scale learning rate by batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7.656495</td>\n",
       "      <td>6.846730</td>\n",
       "      <td>0.056994</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.206811</td>\n",
       "      <td>7.901802</td>\n",
       "      <td>0.038951</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.146518</td>\n",
       "      <td>6.850193</td>\n",
       "      <td>0.056994</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.994226</td>\n",
       "      <td>6.723795</td>\n",
       "      <td>0.056994</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.892519</td>\n",
       "      <td>6.729263</td>\n",
       "      <td>0.056994</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6.824438</td>\n",
       "      <td>6.711360</td>\n",
       "      <td>0.056994</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6.777204</td>\n",
       "      <td>6.704089</td>\n",
       "      <td>0.056994</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>6.742869</td>\n",
       "      <td>6.696327</td>\n",
       "      <td>0.056994</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>6.714705</td>\n",
       "      <td>6.692200</td>\n",
       "      <td>0.056994</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>6.693724</td>\n",
       "      <td>6.691544</td>\n",
       "      <td>0.056994</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/ec2-user/SageMaker/.env/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n"
     ]
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(10, lr, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.to_fp32().save(lm_fns[0], with_opt=False)\n",
    "learn.data.vocab.save(lm_fns[1].with_suffix('.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turkish sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.win.tue.nl/~mpechen/projects/smm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_clas = path/'movies'\n",
    "path_clas.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = (path_clas/'tr_polarity.pos').open(encoding='iso-8859-9').readlines()\n",
    "pos_df = pd.DataFrame({'text':pos})\n",
    "pos_df['pos'] = 1\n",
    "pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = (path_clas/'tr_polarity.neg').open(encoding='iso-8859-9').readlines()\n",
    "neg_df = pd.DataFrame({'text':neg})\n",
    "neg_df['pos'] = 0\n",
    "neg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pos_df,neg_df], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = (TextList.from_df(df, path_clas, cols='text', processor=SPProcessor.load(dest))\n",
    "    .split_by_rand_pct(0.1, seed=42)\n",
    "    .label_for_lm()           \n",
    "    .databunch(bs=bs, num_workers=1))\n",
    "\n",
    "data_lm.save(f'{lang}_clas_databunch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = load_data(path_clas, f'{lang}_clas_databunch', bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm = language_model_learner(data_lm, AWD_LSTM, pretrained_fnames=lm_fns, drop_mult=1.0, wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "lr *= bs/48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm.fit_one_cycle(1, lr*10, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm.unfreeze()\n",
    "learn_lm.fit_one_cycle(5, slice(lr/10,lr*10), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm.save(f'{lang}fine_tuned')\n",
    "learn_lm.save_encoder(f'{lang}fine_tuned_enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = (TextList.from_df(df, path_clas, cols='text', processor=SPProcessor.load(dest))\n",
    "    .split_by_rand_pct(0.1, seed=42)\n",
    "    .label_from_df(cols='pos')\n",
    "    .databunch(bs=bs, num_workers=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_c = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5, pretrained=False, wd=0.1).to_fp16()\n",
    "learn_c.load_encoder(f'{lang}fine_tuned_enc')\n",
    "learn_c.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=2e-2\n",
    "lr *= bs/48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn_c.fit_one_cycle(2, lr, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn_c.fit_one_cycle(2, lr, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_c.freeze_to(-2)\n",
    "learn_c.fit_one_cycle(2, slice(lr/(2.6**4),lr), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_c.freeze_to(-3)\n",
    "learn_c.fit_one_cycle(2, slice(lr/2/(2.6**4),lr/2), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_c.unfreeze()\n",
    "learn_c.fit_one_cycle(4, slice(lr/10/(2.6**4),lr/10), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy in Gezici (2018), *Sentiment Analysis in Turkish* is: `75.16%`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_c.save(f'{lang}clas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
