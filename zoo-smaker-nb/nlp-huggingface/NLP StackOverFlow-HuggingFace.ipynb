{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack Overflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets ipywidgets IProgress sagemaker -qq -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘dataset’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::123456789012:role/service-role/AmazonSageMaker-ExecutionRole-20180314T175147\n",
      "sagemaker bucket: sagemaker-us-east-1-123456789012\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sagemaker_session_bucket}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset # TR NLP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, BertTokenizer, BertModel, BertConfig\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader # MLP , RandomSampler, SequentialSampler\n",
    "\n",
    "from sagemaker.huggingface.model import HuggingFacePredictor\n",
    "\n",
    "pd.set_option('display.max_colwidth', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('softrain5Kv2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Application Development and Application Integration</td>\n",
       "      <td>how to read terraform list from vars fileI have a list of subnets as list in the vars file; which I want to read a list inside the AWS resource.variables.tfvariable &amp;quot;alb_subnets&amp;quot; { type = list(string)}terraform.tfvarssubnets = [&amp;quot;subnet-a&amp;quot;;&amp;quot;subnet-b&amp;quot;]main.tfresource &amp;quot;&amp;lt;resource_name&amp;gt;&amp;quot; &amp;quot;test&amp;quot; {  name               = var.Name  security_groups    = [&amp;quot;${join(&amp;quot;;&amp;quot;;var.subnets)}&amp;quot;]}Error: ValidationError: subnet 'subnet-a;subnet-b' is not valid        status code: 400; request id: dc4be07e-e353-4821-8bde-3d9849584befwhat is the right way to read the list as list inside the AWS resource.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Networking|Other</td>\n",
       "      <td>How to execute a command in (from) zabbix aws elbv2How to execute a command in (from) zabbix UserParameter=alb.co[*];aws elbv2 describe-listener-certificates --listener-arn ******* in sudores.d in file zabbix Defaults:zabbix !requiretty zabbix ALL=NOPASSWD: ALLthe command is not executed.Added credential (macros) to Template&amp;lt;zabbix_export&amp;gt;    &amp;lt;version&amp;gt;5.0&amp;lt;/version&amp;gt;    &amp;lt;date&amp;gt;2020-07-24T10:52:10Z&amp;lt;/date&amp;gt;    &amp;lt;groups&amp;gt;        &amp;lt;group&amp;gt;            &amp;lt;name&amp;gt;Templates&amp;lt;/name&amp;gt;        &amp;lt;/group&amp;gt;    &amp;lt;/groups&amp;gt;    &amp;lt;templates&amp;gt;        &amp;lt;template&amp;gt;            &amp;lt;template&amp;gt;Template alb-co&amp;lt;/template&amp;gt;            &amp;lt;name&amp;gt;Template ssl-count&amp;lt;/name&amp;gt;            &amp;lt;groups&amp;gt;                &amp;lt;group&amp;gt;                    &amp;lt;name&amp;gt;Templates&amp;lt;/name&amp;gt;                &amp;lt;/group&amp;gt;            &amp;lt;/groups&amp;gt;            &amp;lt;applications&amp;gt;                &amp;lt;application&amp;gt;                    &amp;lt;name&amp;gt;alb-co&amp;lt;/name&amp;gt;                &amp;lt;/application&amp;gt;            &amp;lt;/applications&amp;gt;            &amp;lt;items&amp;gt;                &amp;lt;item&amp;gt;                    &amp;lt;name&amp;gt;alb-co&amp;lt;/name&amp;gt;                    &amp;lt;key&amp;gt;alb.co[arn;{$HOST.DNS};--access-key{$AWS_ACCESS_KEY};--secret-key{$AWS_SECRET_KEY};--region{$REGION}]&amp;lt;/key&amp;gt;                    &amp;lt;delay&amp;gt;30s&amp;lt;/delay&amp;gt;                    &amp;lt;applications&amp;gt;                        &amp;lt;application&amp;gt;                            &amp;lt;name&amp;gt;alb-co&amp;lt;/name&amp;gt;                        &amp;lt;/application&amp;gt;                    &amp;lt;/applications&amp;gt;                    &amp;lt;triggers&amp;gt;                        &amp;lt;trigger&amp;gt;                            &amp;lt;expression&amp;gt;{last()}&amp;amp;gt;=23&amp;lt;/expression&amp;gt;                            &amp;lt;name&amp;gt;{ITEM.LASTVALUE} alb-co {HOST.DNS}&amp;lt;/name&amp;gt;                            &amp;lt;priority&amp;gt;INFO&amp;lt;/priority&amp;gt;                            &amp;lt;manual_close&amp;gt;YES&amp;lt;/manual_close&amp;gt;&amp;lt;/trigger&amp;gt;                    &amp;lt;/triggers&amp;gt;                &amp;lt;/item&amp;gt;            &amp;lt;/items&amp;gt;            &amp;lt;macros&amp;gt;                &amp;lt;macro&amp;gt;                    &amp;lt;macro&amp;gt;{$AWS_ACCESS_KEY}&amp;lt;/macro&amp;gt;                    &amp;lt;type&amp;gt;SECRET_TEXT&amp;lt;/type&amp;gt;                &amp;lt;/macro&amp;gt;                &amp;lt;macro&amp;gt;                    &amp;lt;macro&amp;gt;{$AWS_SECRET_KEY}&amp;lt;/macro&amp;gt;                    &amp;lt;type&amp;gt;SECRET_TEXT&amp;lt;/type&amp;gt;                &amp;lt;/macro&amp;gt;                &amp;lt;macro&amp;gt;                    &amp;lt;macro&amp;gt;{$REGION}&amp;lt;/macro&amp;gt;                    &amp;lt;value&amp;gt;eu-central-1&amp;lt;/value&amp;gt;                &amp;lt;/macro&amp;gt;            &amp;lt;/macros&amp;gt;        &amp;lt;/template&amp;gt;    &amp;lt;/templates&amp;gt;&amp;lt;/zabbix_export&amp;gt;But still doesn't work.There are no item and trigger errors; it just returns the value 0;but should 26. Maybe someone started with this?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                labels  \\\n",
       "0  Application Development and Application Integration   \n",
       "1  Networking|Other                                      \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            source  \n",
       "0  how to read terraform list from vars fileI have a list of subnets as list in the vars file; which I want to read a list inside the AWS resource.variables.tfvariable &quot;alb_subnets&quot; { type = list(string)}terraform.tfvarssubnets = [&quot;subnet-a&quot;;&quot;subnet-b&quot;]main.tfresource &quot;&lt;resource_name&gt;&quot; &quot;test&quot; {  name               = var.Name  security_groups    = [&quot;${join(&quot;;&quot;;var.subnets)}&quot;]}Error: ValidationError: subnet 'subnet-a;subnet-b' is not valid        status code: 400; request id: dc4be07e-e353-4821-8bde-3d9849584befwhat is the right way to read the list as list inside the AWS resource.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "1  How to execute a command in (from) zabbix aws elbv2How to execute a command in (from) zabbix UserParameter=alb.co[*];aws elbv2 describe-listener-certificates --listener-arn ******* in sudores.d in file zabbix Defaults:zabbix !requiretty zabbix ALL=NOPASSWD: ALLthe command is not executed.Added credential (macros) to Template&lt;zabbix_export&gt;    &lt;version&gt;5.0&lt;/version&gt;    &lt;date&gt;2020-07-24T10:52:10Z&lt;/date&gt;    &lt;groups&gt;        &lt;group&gt;            &lt;name&gt;Templates&lt;/name&gt;        &lt;/group&gt;    &lt;/groups&gt;    &lt;templates&gt;        &lt;template&gt;            &lt;template&gt;Template alb-co&lt;/template&gt;            &lt;name&gt;Template ssl-count&lt;/name&gt;            &lt;groups&gt;                &lt;group&gt;                    &lt;name&gt;Templates&lt;/name&gt;                &lt;/group&gt;            &lt;/groups&gt;            &lt;applications&gt;                &lt;application&gt;                    &lt;name&gt;alb-co&lt;/name&gt;                &lt;/application&gt;            &lt;/applications&gt;            &lt;items&gt;                &lt;item&gt;                    &lt;name&gt;alb-co&lt;/name&gt;                    &lt;key&gt;alb.co[arn;{$HOST.DNS};--access-key{$AWS_ACCESS_KEY};--secret-key{$AWS_SECRET_KEY};--region{$REGION}]&lt;/key&gt;                    &lt;delay&gt;30s&lt;/delay&gt;                    &lt;applications&gt;                        &lt;application&gt;                            &lt;name&gt;alb-co&lt;/name&gt;                        &lt;/application&gt;                    &lt;/applications&gt;                    &lt;triggers&gt;                        &lt;trigger&gt;                            &lt;expression&gt;{last()}&amp;gt;=23&lt;/expression&gt;                            &lt;name&gt;{ITEM.LASTVALUE} alb-co {HOST.DNS}&lt;/name&gt;                            &lt;priority&gt;INFO&lt;/priority&gt;                            &lt;manual_close&gt;YES&lt;/manual_close&gt;&lt;/trigger&gt;                    &lt;/triggers&gt;                &lt;/item&gt;            &lt;/items&gt;            &lt;macros&gt;                &lt;macro&gt;                    &lt;macro&gt;{$AWS_ACCESS_KEY}&lt;/macro&gt;                    &lt;type&gt;SECRET_TEXT&lt;/type&gt;                &lt;/macro&gt;                &lt;macro&gt;                    &lt;macro&gt;{$AWS_SECRET_KEY}&lt;/macro&gt;                    &lt;type&gt;SECRET_TEXT&lt;/type&gt;                &lt;/macro&gt;                &lt;macro&gt;                    &lt;macro&gt;{$REGION}&lt;/macro&gt;                    &lt;value&gt;eu-central-1&lt;/value&gt;                &lt;/macro&gt;            &lt;/macros&gt;        &lt;/template&gt;    &lt;/templates&gt;&lt;/zabbix_export&gt;But still doesn't work.There are no item and trigger errors; it just returns the value 0;but should 26. Maybe someone started with this?  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'] = df['labels'].str.split('|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0]\n",
      "[('Application Development and Application Integration',)]\n",
      "['Analytics and AI/ML'\n",
      " 'Application Development and Application Integration' 'Compute'\n",
      " 'Database and Storage' 'Governance, Security and Ops' 'Networking'\n",
      " 'Other']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "yt = mlb.fit_transform(df['labels'])\n",
    "# Getting a sense of how the tags data looks like\n",
    "print(yt[0])\n",
    "print(mlb.inverse_transform(yt[0].reshape(1,-1)))\n",
    "print(mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfy = pd.DataFrame(yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfy['combined']= dfy.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ymlblabels'] = dfy['combined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['labels'], axis=1, inplace=True)\n",
    "df =  df.rename(columns={\"source\": \"comment_text\", \"ymlblabels\": \"target_classification\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>target_classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how to read terraform list from vars fileI have a list of subnets as list in the vars file; which I want to read a list inside the AWS resource.variables.tfvariable &amp;quot;alb_subnets&amp;quot; { type = list(string)}terraform.tfvarssubnets = [&amp;quot;subnet-a&amp;quot;;&amp;quot;subnet-b&amp;quot;]main.tfresource &amp;quot;&amp;lt;resource_name&amp;gt;&amp;quot; &amp;quot;test&amp;quot; {  name               = var.Name  security_groups    = [&amp;quot;${join(&amp;quot;;&amp;quot;;var.subnets)}&amp;quot;]}Error: ValidationError: subnet 'subnet-a;subnet-b' is not valid        status code: 400; request id: dc4be07e-e353-4821-8bde-3d9849584befwhat is the right way to read the list as list inside the AWS resource.</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How to execute a command in (from) zabbix aws elbv2How to execute a command in (from) zabbix UserParameter=alb.co[*];aws elbv2 describe-listener-certificates --listener-arn ******* in sudores.d in file zabbix Defaults:zabbix !requiretty zabbix ALL=NOPASSWD: ALLthe command is not executed.Added credential (macros) to Template&amp;lt;zabbix_export&amp;gt;    &amp;lt;version&amp;gt;5.0&amp;lt;/version&amp;gt;    &amp;lt;date&amp;gt;2020-07-24T10:52:10Z&amp;lt;/date&amp;gt;    &amp;lt;groups&amp;gt;        &amp;lt;group&amp;gt;            &amp;lt;name&amp;gt;Templates&amp;lt;/name&amp;gt;        &amp;lt;/group&amp;gt;    &amp;lt;/groups&amp;gt;    &amp;lt;templates&amp;gt;        &amp;lt;template&amp;gt;            &amp;lt;template&amp;gt;Template alb-co&amp;lt;/template&amp;gt;            &amp;lt;name&amp;gt;Template ssl-count&amp;lt;/name&amp;gt;            &amp;lt;groups&amp;gt;                &amp;lt;group&amp;gt;                    &amp;lt;name&amp;gt;Templates&amp;lt;/name&amp;gt;                &amp;lt;/group&amp;gt;            &amp;lt;/groups&amp;gt;            &amp;lt;applications&amp;gt;                &amp;lt;application&amp;gt;                    &amp;lt;name&amp;gt;alb-co&amp;lt;/name&amp;gt;                &amp;lt;/application&amp;gt;            &amp;lt;/applications&amp;gt;            &amp;lt;items&amp;gt;                &amp;lt;item&amp;gt;                    &amp;lt;name&amp;gt;alb-co&amp;lt;/name&amp;gt;                    &amp;lt;key&amp;gt;alb.co[arn;{$HOST.DNS};--access-key{$AWS_ACCESS_KEY};--secret-key{$AWS_SECRET_KEY};--region{$REGION}]&amp;lt;/key&amp;gt;                    &amp;lt;delay&amp;gt;30s&amp;lt;/delay&amp;gt;                    &amp;lt;applications&amp;gt;                        &amp;lt;application&amp;gt;                            &amp;lt;name&amp;gt;alb-co&amp;lt;/name&amp;gt;                        &amp;lt;/application&amp;gt;                    &amp;lt;/applications&amp;gt;                    &amp;lt;triggers&amp;gt;                        &amp;lt;trigger&amp;gt;                            &amp;lt;expression&amp;gt;{last()}&amp;amp;gt;=23&amp;lt;/expression&amp;gt;                            &amp;lt;name&amp;gt;{ITEM.LASTVALUE} alb-co {HOST.DNS}&amp;lt;/name&amp;gt;                            &amp;lt;priority&amp;gt;INFO&amp;lt;/priority&amp;gt;                            &amp;lt;manual_close&amp;gt;YES&amp;lt;/manual_close&amp;gt;&amp;lt;/trigger&amp;gt;                    &amp;lt;/triggers&amp;gt;                &amp;lt;/item&amp;gt;            &amp;lt;/items&amp;gt;            &amp;lt;macros&amp;gt;                &amp;lt;macro&amp;gt;                    &amp;lt;macro&amp;gt;{$AWS_ACCESS_KEY}&amp;lt;/macro&amp;gt;                    &amp;lt;type&amp;gt;SECRET_TEXT&amp;lt;/type&amp;gt;                &amp;lt;/macro&amp;gt;                &amp;lt;macro&amp;gt;                    &amp;lt;macro&amp;gt;{$AWS_SECRET_KEY}&amp;lt;/macro&amp;gt;                    &amp;lt;type&amp;gt;SECRET_TEXT&amp;lt;/type&amp;gt;                &amp;lt;/macro&amp;gt;                &amp;lt;macro&amp;gt;                    &amp;lt;macro&amp;gt;{$REGION}&amp;lt;/macro&amp;gt;                    &amp;lt;value&amp;gt;eu-central-1&amp;lt;/value&amp;gt;                &amp;lt;/macro&amp;gt;            &amp;lt;/macros&amp;gt;        &amp;lt;/template&amp;gt;    &amp;lt;/templates&amp;gt;&amp;lt;/zabbix_export&amp;gt;But still doesn't work.There are no item and trigger errors; it just returns the value 0;but should 26. Maybe someone started with this?</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      comment_text  \\\n",
       "0  how to read terraform list from vars fileI have a list of subnets as list in the vars file; which I want to read a list inside the AWS resource.variables.tfvariable &quot;alb_subnets&quot; { type = list(string)}terraform.tfvarssubnets = [&quot;subnet-a&quot;;&quot;subnet-b&quot;]main.tfresource &quot;&lt;resource_name&gt;&quot; &quot;test&quot; {  name               = var.Name  security_groups    = [&quot;${join(&quot;;&quot;;var.subnets)}&quot;]}Error: ValidationError: subnet 'subnet-a;subnet-b' is not valid        status code: 400; request id: dc4be07e-e353-4821-8bde-3d9849584befwhat is the right way to read the list as list inside the AWS resource.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "1  How to execute a command in (from) zabbix aws elbv2How to execute a command in (from) zabbix UserParameter=alb.co[*];aws elbv2 describe-listener-certificates --listener-arn ******* in sudores.d in file zabbix Defaults:zabbix !requiretty zabbix ALL=NOPASSWD: ALLthe command is not executed.Added credential (macros) to Template&lt;zabbix_export&gt;    &lt;version&gt;5.0&lt;/version&gt;    &lt;date&gt;2020-07-24T10:52:10Z&lt;/date&gt;    &lt;groups&gt;        &lt;group&gt;            &lt;name&gt;Templates&lt;/name&gt;        &lt;/group&gt;    &lt;/groups&gt;    &lt;templates&gt;        &lt;template&gt;            &lt;template&gt;Template alb-co&lt;/template&gt;            &lt;name&gt;Template ssl-count&lt;/name&gt;            &lt;groups&gt;                &lt;group&gt;                    &lt;name&gt;Templates&lt;/name&gt;                &lt;/group&gt;            &lt;/groups&gt;            &lt;applications&gt;                &lt;application&gt;                    &lt;name&gt;alb-co&lt;/name&gt;                &lt;/application&gt;            &lt;/applications&gt;            &lt;items&gt;                &lt;item&gt;                    &lt;name&gt;alb-co&lt;/name&gt;                    &lt;key&gt;alb.co[arn;{$HOST.DNS};--access-key{$AWS_ACCESS_KEY};--secret-key{$AWS_SECRET_KEY};--region{$REGION}]&lt;/key&gt;                    &lt;delay&gt;30s&lt;/delay&gt;                    &lt;applications&gt;                        &lt;application&gt;                            &lt;name&gt;alb-co&lt;/name&gt;                        &lt;/application&gt;                    &lt;/applications&gt;                    &lt;triggers&gt;                        &lt;trigger&gt;                            &lt;expression&gt;{last()}&amp;gt;=23&lt;/expression&gt;                            &lt;name&gt;{ITEM.LASTVALUE} alb-co {HOST.DNS}&lt;/name&gt;                            &lt;priority&gt;INFO&lt;/priority&gt;                            &lt;manual_close&gt;YES&lt;/manual_close&gt;&lt;/trigger&gt;                    &lt;/triggers&gt;                &lt;/item&gt;            &lt;/items&gt;            &lt;macros&gt;                &lt;macro&gt;                    &lt;macro&gt;{$AWS_ACCESS_KEY}&lt;/macro&gt;                    &lt;type&gt;SECRET_TEXT&lt;/type&gt;                &lt;/macro&gt;                &lt;macro&gt;                    &lt;macro&gt;{$AWS_SECRET_KEY}&lt;/macro&gt;                    &lt;type&gt;SECRET_TEXT&lt;/type&gt;                &lt;/macro&gt;                &lt;macro&gt;                    &lt;macro&gt;{$REGION}&lt;/macro&gt;                    &lt;value&gt;eu-central-1&lt;/value&gt;                &lt;/macro&gt;            &lt;/macros&gt;        &lt;/template&gt;    &lt;/templates&gt;&lt;/zabbix_export&gt;But still doesn't work.There are no item and trigger errors; it just returns the value 0;but should 26. Maybe someone started with this?   \n",
       "\n",
       "   target_classification  \n",
       "0  [0, 1, 0, 0, 0, 0, 0]  \n",
       "1  [0, 0, 0, 0, 0, 1, 1]  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('softrain5Ktrain.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset\n",
    "https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sections of config\n",
    "\n",
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 200\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.comment_text = dataframe.comment_text\n",
    "        self.targets = self.data.target_classification\n",
    "        self.max_len = max_len\n",
    "#         print('HELLOOOOOO INIT')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comment_text)\n",
    "    \n",
    "    def data(self):\n",
    "        return self._data\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        comment_text = str(self.comment_text[index])\n",
    "#         print(comment_text)\n",
    "        comment_text = \" \".join(comment_text.split())\n",
    " \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            comment_text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (5000, 2)\n",
      "TRAIN Dataset: (4500, 2)\n",
      "TEST Dataset: (500, 2)\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "\n",
    "train_size = 0.9\n",
    "train_dataset=df.sample(frac=train_size,random_state=200)\n",
    "test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom NLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return_dict=False https://stackoverflow.com/questions/65082243/dropout-argument-input-position-1-must-be-tensor-not-str-when-using-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (l1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (l2): Dropout(p=0.3, inplace=False)\n",
       "  (l3): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 7)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "\n",
    "model = BERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "        \n",
    "#         print(f\"\\n======================\\n\\nIDs-----\\n{ids}\\n-----MASK-----\\n{mask}\\n-----\\nTOKEN TYPE IDs----{token_type_ids}\\n\\n\")\n",
    "\n",
    "#         print(\"\\n *********** SHAPE ids {}\\n\".format(ids.shape))\n",
    "#         print(\"\\n *********** SHAPE mask {}\\n\".format(mask.shape))\n",
    "#         print(\"\\n *********** SHAPE token_type_ids {}\\n\".format(token_type_ids.shape))\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        if _%100==0:\n",
    "            print(f'Epoch: {epoch}, Batch: {_}, Loss:  {loss.item()}')\n",
    "#         print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/opt/conda/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss:  0.7606750726699829\n",
      "Epoch: 0, Batch: 100, Loss:  0.5396038889884949\n",
      "Epoch: 0, Batch: 200, Loss:  0.5140712261199951\n",
      "Epoch: 0, Batch: 300, Loss:  0.5634191036224365\n",
      "Epoch: 0, Batch: 400, Loss:  0.4400113821029663\n",
      "Epoch: 0, Batch: 500, Loss:  0.37731286883354187\n",
      "Epoch: 1, Batch: 0, Loss:  0.3461363911628723\n",
      "Epoch: 1, Batch: 100, Loss:  0.3563036322593689\n",
      "Epoch: 1, Batch: 200, Loss:  0.32605138421058655\n",
      "Epoch: 1, Batch: 300, Loss:  0.37069427967071533\n",
      "Epoch: 1, Batch: 400, Loss:  0.41086670756340027\n",
      "Epoch: 1, Batch: 500, Loss:  0.3334399163722992\n",
      "Epoch: 2, Batch: 0, Loss:  0.292452871799469\n",
      "Epoch: 2, Batch: 100, Loss:  0.29512399435043335\n",
      "Epoch: 2, Batch: 200, Loss:  0.1622379720211029\n",
      "Epoch: 2, Batch: 300, Loss:  0.25359535217285156\n",
      "Epoch: 2, Batch: 400, Loss:  0.2948223054409027\n",
      "Epoch: 2, Batch: 500, Loss:  0.29760852456092834\n",
      "Epoch: 3, Batch: 0, Loss:  0.23609690368175507\n",
      "Epoch: 3, Batch: 100, Loss:  0.15604087710380554\n",
      "Epoch: 3, Batch: 200, Loss:  0.22960960865020752\n",
      "Epoch: 3, Batch: 300, Loss:  0.2256130874156952\n",
      "Epoch: 3, Batch: 400, Loss:  0.27050837874412537\n",
      "Epoch: 3, Batch: 500, Loss:  0.3242933452129364\n",
      "Epoch: 4, Batch: 0, Loss:  0.2280043661594391\n",
      "Epoch: 4, Batch: 100, Loss:  0.21593452990055084\n",
      "Epoch: 4, Batch: 200, Loss:  0.1718410700559616\n",
      "Epoch: 4, Batch: 300, Loss:  0.2372029721736908\n",
      "Epoch: 4, Batch: 400, Loss:  0.12450963258743286\n",
      "Epoch: 4, Batch: 500, Loss:  0.2718450427055359\n",
      "Epoch: 5, Batch: 0, Loss:  0.14271026849746704\n",
      "Epoch: 5, Batch: 100, Loss:  0.18141907453536987\n",
      "Epoch: 5, Batch: 200, Loss:  0.147258460521698\n",
      "Epoch: 5, Batch: 300, Loss:  0.1693379282951355\n",
      "Epoch: 5, Batch: 400, Loss:  0.10361514240503311\n",
      "Epoch: 5, Batch: 500, Loss:  0.18840259313583374\n",
      "Epoch: 6, Batch: 0, Loss:  0.1074734777212143\n",
      "Epoch: 6, Batch: 100, Loss:  0.08634157478809357\n",
      "Epoch: 6, Batch: 200, Loss:  0.22527307271957397\n",
      "Epoch: 6, Batch: 300, Loss:  0.26230916380882263\n",
      "Epoch: 6, Batch: 400, Loss:  0.15703387558460236\n",
      "Epoch: 6, Batch: 500, Loss:  0.09252019226551056\n",
      "Epoch: 7, Batch: 0, Loss:  0.08274565637111664\n",
      "Epoch: 7, Batch: 100, Loss:  0.13026678562164307\n",
      "Epoch: 7, Batch: 200, Loss:  0.10377724468708038\n",
      "Epoch: 7, Batch: 300, Loss:  0.07639417797327042\n",
      "Epoch: 7, Batch: 400, Loss:  0.12655365467071533\n",
      "Epoch: 7, Batch: 500, Loss:  0.07652541995048523\n",
      "Epoch: 8, Batch: 0, Loss:  0.11221946775913239\n",
      "Epoch: 8, Batch: 100, Loss:  0.07605963200330734\n",
      "Epoch: 8, Batch: 200, Loss:  0.0623369999229908\n",
      "Epoch: 8, Batch: 300, Loss:  0.0638340637087822\n",
      "Epoch: 8, Batch: 400, Loss:  0.07765527814626694\n",
      "Epoch: 8, Batch: 500, Loss:  0.05323268100619316\n",
      "Epoch: 9, Batch: 0, Loss:  0.09693458676338196\n",
      "Epoch: 9, Batch: 100, Loss:  0.052944328635931015\n",
      "Epoch: 9, Batch: 200, Loss:  0.07574012875556946\n",
      "Epoch: 9, Batch: 300, Loss:  0.07285337895154953\n",
      "Epoch: 9, Batch: 400, Loss:  0.05613085627555847\n",
      "Epoch: 9, Batch: 500, Loss:  0.06000782921910286\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    return fin_outputs, fin_targets, ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About model scoring, multi label: https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(EPOCHS):\n",
    "#     outputs, targets = validation(epoch)\n",
    "# #     print(targets)\n",
    "# #     print(outputs)\n",
    "#     outputs = np.array(outputs) >= 0.5\n",
    "#     accuracy = metrics.accuracy_score(targets, outputs)\n",
    "#     f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "#     f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "#     f1_score_weighted = metrics.f1_score(targets, outputs, average='weighted')\n",
    "#     f1_score_sample = metrics.f1_score(targets, outputs, average='samples')\n",
    "#     f1_score_none = metrics.f1_score(targets, outputs, average=None)\n",
    "#     precision = metrics.precision_score(targets, outputs, average='samples')\n",
    "#     recall = metrics.recall_score(targets, outputs, average='samples')\n",
    "\n",
    "#     print(f\"Accuracy Score = {accuracy}\")\n",
    "#     print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "#     print(f\"F1 Score (Macro) = {f1_score_macro}\")\n",
    "#     print(f\"F1 Score (Weighted) = {f1_score_weighted}\")\n",
    "#     print(f\"F1 Score (Sample) = {f1_score_sample}\")\n",
    "#     print(f\"F1 Score (None) = {f1_score_none}\")\n",
    "#     print('[0: Analytics and AI/ML 1:Application Development and Application Integration 2:Compute 3: Database and Storage 4: Governance Security and Ops 5: Networking 6: Other]')\n",
    "#     print(f\"Precision Score (Samples) = {precision}\")\n",
    "#     print(f\"Recall Score (Samples) = {recall}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.338\n",
      "F1 Score (Micro) = 0.678809957498482\n",
      "F1 Score (Macro) = 0.664171382585759\n",
      "F1 Score (Weighted) = 0.6706835647203292\n",
      "F1 Score (Sample) = 0.6856666666666668\n",
      "F1 Score (None) = [0.78571429 0.61096606 0.75625    0.80808081 0.60223048 0.70063694\n",
      " 0.3853211 ]\n",
      "[0: Analytics and AI/ML 1:Application Development and Application Integration 2:Compute 3: Database and Storage 4: Governance Security and Ops 5: Networking 6: Other]\n",
      "Precision Score (Samples) = 0.7032333333333334\n",
      "Recall Score (Samples) = 0.736\n"
     ]
    }
   ],
   "source": [
    "# for epoch in range(EPOCHS):\n",
    "outputs, targets, comment_ids = validation(epoch)\n",
    "#     print(targets)\n",
    "#     print(outputs)\n",
    "outputs = np.array(outputs) >= 0.5\n",
    "accuracy = metrics.accuracy_score(targets, outputs)\n",
    "f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "f1_score_weighted = metrics.f1_score(targets, outputs, average='weighted')\n",
    "f1_score_sample = metrics.f1_score(targets, outputs, average='samples')\n",
    "f1_score_none = metrics.f1_score(targets, outputs, average=None)\n",
    "precision = metrics.precision_score(targets, outputs, average='samples')\n",
    "recall = metrics.recall_score(targets, outputs, average='samples')\n",
    "\n",
    "print(f\"Accuracy Score = {accuracy}\")\n",
    "print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "print(f\"F1 Score (Macro) = {f1_score_macro}\")\n",
    "print(f\"F1 Score (Weighted) = {f1_score_weighted}\")\n",
    "print(f\"F1 Score (Sample) = {f1_score_sample}\")\n",
    "print(f\"F1 Score (None) = {f1_score_none}\")\n",
    "print('[0: Analytics and AI/ML 1:Application Development and Application Integration 2:Compute 3: Database and Storage 4: Governance Security and Ops 5: Networking 6: Other]')\n",
    "print(f\"Precision Score (Samples) = {precision}\")\n",
    "print(f\"Recall Score (Samples) = {recall}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     precision    recall  f1-score   support\n",
      "\n",
      "                                Analytics and AI/ML       0.80      0.77      0.79        57\n",
      "Application Development and Application Integration       0.59      0.64      0.61       184\n",
      "                                            Compute       0.68      0.85      0.76       143\n",
      "                               Database and Storage       0.75      0.88      0.81       137\n",
      "                       Governance, Security and Ops       0.55      0.67      0.60       121\n",
      "                                         Networking       0.74      0.66      0.70        83\n",
      "                                              Other       0.58      0.29      0.39        73\n",
      "\n",
      "                                          micro avg       0.66      0.70      0.68       798\n",
      "                                          macro avg       0.67      0.68      0.66       798\n",
      "                                       weighted avg       0.66      0.70      0.67       798\n",
      "                                        samples avg       0.70      0.74      0.69       798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = ['Analytics and AI/ML', 'Application Development and Application Integration', 'Compute', \n",
    "                'Database and Storage', 'Governance, Security and Ops', 'Networking', 'Other']\n",
    "print(classification_report(targets, outputs, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2003,  2045,  5724,  2000,  2417,  7442,  6593,  8299,  4026,\n",
       "          2000, 16770,  1999, 22091,  2015,  2897,  7170,  5703,  6657,  4438,\n",
       "          7170,  5703,  2099,  1006, 18856,  2497,  1007,  1998,  4646,  7170,\n",
       "          5703,  2099,  1006,  2632,  2497,  1007,  2045,  2003,  5724,  2000,\n",
       "          2417,  7442,  6593,  2035,  8299,  4026,  2000, 16770, 19373,  1012,\n",
       "          1045,  2079,  2025,  2424,  1996,  5724,  2000,  2417,  7442,  6593,\n",
       "         22975,  2361,  3417,  3770,  4026,  2000,  1056,  4877,  3417,  4008,\n",
       "          2509,  2013, 17953,  2497,  1006,  2897,  7170,  5703,  2099,  1007,\n",
       "          2151,  2393,  2003,  2172, 12315,  1012,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_ids[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True,  True, False, False, False, False],\n",
       "       [False,  True, False, False,  True, False, False],\n",
       "       [False,  True, False,  True, False, False, False],\n",
       "       [False, False, False, False,  True, False, False],\n",
       "       [False, False, False,  True, False, False, False]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/datasets/quicktour.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(testing_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample ----- 0\n",
      "[CLS] is there option to redirect http traffic to https in aws network load balancerin classic load balancer ( clb ) and application load balancer ( alb ) there is option to redirect all http traffic to https listener. i do not find the option to redirect tcp port 80 traffic to tls port 443 from nlb ( network load balancer ) any help is much appreciated. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Data: Targets [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Data: Outputs [False False  True False False False False]\n",
      "Sample ----- 1\n",
      "[CLS] serve image using s3 and express in lambda projecti have a bucket where i save all my user related files and i want the user to be able to download those files ; the problem i have is that i need to proxy the petition via an api url likehttps : / / myproject. com / api / file / : keywhere : key is the object key in the s3 ; that way i hide the s3 bucket to my user and the url is more friendly. the result should be something like this if i click the link ; it opens a browser tab and display the imagei have tried this const s3 = new aws. s3 ( ) ; const s3params = { bucket : bucket ; key : key ; } ; const data = s3. getobject ( s3params ). promise ( ) ; res. download ( data ;'file. jpg') ; / [SEP]\n",
      "Data: Targets [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "Data: Outputs [False  True False  True False False False]\n",
      "Sample ----- 2\n",
      "[CLS] what is the limit of number of keys i can import in java keystore? i have a scenario where i need to store many private / public keys in java keystore instance and use them for various purpose. number of keys that i store in here could grow to thousands. is it a better approach to store / load huge number of keys into keystore? and if not ; what could be other better options? my application runs on aws and i will have multiple instances running that would need the keys from this keystore. each instance having separate keystore instance could work ; but is there also any option to share the keystore instance across my application instances? i have explored aws certificate manager and somehow it does not fulfill my requirement. i want a keystore to work as a database for public / private keys but i have large number of keys. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Data: Targets [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "Data: Outputs [False  True False False  True False False]\n",
      "Sample ----- 3\n",
      "[CLS] uploading file which is larger than 1mb to s3 ; getting cors error. no & # 39 ; access - control - allow - origin & # 39 ; header is present on the requested resourcei am trying to upload an image to s3 with express and multer. my frontend was deployed through aws amplify and the backend through elastic beanstalk. the thing is when i tried to upload an image of less than 1mb ; it works fine. but if the file is larger than 1mb i got the error below. access to xmlhttprequest at'my backend route'from origin'my frontend route'has been blocked by cors policy : no'access - control - allow - origin'header is present on the requested resource. i thought the problem was about the nginx limited body file size ; so i changed it ; but it was not. 4mb image file upload gives http 413 error [SEP]\n",
      "Data: Targets [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "Data: Outputs [False False False  True False False False]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(comment_ids)):\n",
    "    print(f\"Sample ----- {i}\")\n",
    "    print(tokenizer.decode(comment_ids[i]))\n",
    "    print(\"Data: Targets\", targets[i])\n",
    "    print(\"Data: Outputs\", outputs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data IDs  tensor([  101,  2025,  2583,  2000,  9699,  9230,  2015,  1999,  8946,  2121,\n",
      "         6520, 23422,  4646,  2072,  2572,  2667,  2000,  2448,  9230,  2015,\n",
      "         1999,  1996,  6520, 23422,  4646,  2008,  2003,  2478,  8946,  2121,\n",
      "         2021,  1996,  3431,  2024,  2025, 10842,  1012,  1045,  2018,  2053,\n",
      "         3277,  2007,  1996,  3988,  9230,  2015,  2021,  2044,  2008,  1025,\n",
      "         2065,  1045,  2191,  2151,  3431,  2000,  2026,  4275,  2008,  2024,\n",
      "         2025,  7686,  1999,  1996,  9230,  2015,  1998,  2009,  7906, 11717,\n",
      "         3988,  9230,  2015,  1012,  1045,  2572,  2025,  2583,  2000,  2424,\n",
      "         2151,  9230,  5371,  2205,  2672,  2008,  2003,  4786,  2023,  3291,\n",
      "         2021,  1045,  2572,  2025,  2583,  2000,  2424,  2151,  5576,  1012,\n",
      "         1045,  2572,  2478,  9733, 16428,  2015,  2004,  7809,  1998,  4646,\n",
      "         2003,  2036,  4354,  2006, 22091,  2015,  1998,  1045,  2572,  2478,\n",
      "         2023,  3094,  2005,  9230,  2015,  1012,  8946,  2121,  1011, 17202,\n",
      "         1011,  1042,  2537,  1012,  1061, 19968,  2448,  6520, 23422, 18750,\n",
      "         6133,  1012,  1052,  2100,  2191,  4328, 29397,  2015, 10439,  1035,\n",
      "         2171,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "[CLS] not able to generate migrations in docker django applicationi am trying to run migrations in the django application that is using docker but the changes are not reflecting. i had no issue with the initial migrations but after that ; if i make any changes to my models that are not reflected in the migrations and it keeps generating initial migrations. i am not able to find any migration file too maybe that is causing this problem but i am not able to find any solution. i am using amazon rds as database and application is also hosted on aws and i am using this command for migrations. docker - compose - f production. yml run django python manage. py makemigrations app _ name [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Data: Targets tensor([0., 1., 1., 0., 0., 0., 0.])\n",
      "Data IDs  tensor([  101, 22091,  2015, 25238, 13529,  2121, 18687,  1046,  3385,  9140,\n",
      "         6309,  2004,  2358,  6820,  6593,  2072,  2031, 26928,  1037, 13529,\n",
      "         2121,  1999, 22091,  2015, 25238,  2000,  3443,  1037,  2795,  2013,\n",
      "         9089,  2098,  1046,  3385,  8833,  6764,  1012,  2021,  1996,  1046,\n",
      "         3385,  6764,  5383,  2019,  9140,  3145,  1006,  1057, 21272,  1007,\n",
      "         2008,  2003,  5068,  2004,  2112,  1997,  1996,  3252,  1012,  2079,\n",
      "         3087,  2113,  2065,  2009,  2003,  2825,  2000, 13558,  1996,  9140,\n",
      "         6309,  1006,  1057, 21272,  1007,  1999,  1996,  2795,  3252,  1029,\n",
      "         2138,  2065,  2057,  2031,  2525,  2580,  1037,  2795,  2011, 13529,\n",
      "         2121,  1025,  1996, 10651, 13529,  2121,  2131,  1037,  1000,  4722,\n",
      "         6453,  7561,  1000,  4471,  1012,  1046,  3385,  3252,  1063,  1000,\n",
      "         4431,  1000,  1024,  1000,  1042, 23632,  2050,  2575,  2278,  2575,\n",
      "         2050,  1000,  1025,  1000,  5167,  1000,  1024,  1063,  1000, 17405,\n",
      "         2575,  2581,  2094,  2683,  2050,  2620,  2683, 10354,  1000,  1024,\n",
      "         1063,  1000,  8909,  1000,  1024,  1000, 17405,  2575,  2581,  2094,\n",
      "         2683,  2050,  2620,  2683, 10354,  1000,  1025,  1000,  2171,  1000,\n",
      "         1024,  1000,  4031, 18442,  1000,  1065,  1065, 12105,  3252,  2890,\n",
      "        25523,  4221,  5244,  1011, 17405,  2575,  2581,  2094,  2683,  2050,\n",
      "         2620,  2683, 10354,  1011,  1011,  8909,  1011,  1011,  2315,  2229,\n",
      "        27559,  9560,  2890, 25523,  4221,  5244,  1011,  8909,  1011,   102])\n",
      "[CLS] aws glue crawler registers json array keys as structi have configured a crawler in aws glue to create a table from nested json log files. but the json files contain an array key ( uuid ) that is registered as part of the structure. do anyone know if it is possible to skip the array keys ( uuid ) in the table structure? because if we have already created a table by crawler ; the update crawler get a \" internal exception error \" message. json structure { \" reference \" : \" f41a6c6a \" ; \" items \" : { \" 21567d9a89af \" : { \" id \" : \" 21567d9a89af \" ; \" name \" : \" productname \" } } catalog structurereferenceitems - 21567d9a89af - - id - - namedesired outcomereferenceitems - id - [SEP]\n",
      "Data: Targets tensor([1., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for datax in testing_set:\n",
    "#   print(\"Data: \", datax) # ids, mask, token_type_ids, targets\n",
    "  print(\"Data IDs \", datax['ids'])\n",
    "#   print(\"Waveform: {}\\nSample rate: \\nLabels: \".format(datax[0] )) # , datax[1])) # , datax[2]))\n",
    "  print(tokenizer.decode(datax['ids']))\n",
    "  print(\"Data: Targets\", datax['targets'])\n",
    "\n",
    "#   print(\"Data: \", datax['ids'][2])\n",
    "\n",
    "  if i == 1:\n",
    "      break\n",
    "  i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# model.train().to(device)\n",
    "# optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)\n",
    "# for epoch in range(1):\n",
    "#     for i, batch in enumerate(tqdm(training_loader)):\n",
    "#         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#         outputs = model(**batch)\n",
    "#         loss = outputs[0]\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "#         if i % 10 == 0:\n",
    "#             print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data IDs  tensor([  101, 22091,  2015, 15775, 13231,  2334,  2573,  2021,  2025, 15775,\n",
      "        13231, 21296,  2072,  2572,  3492,  2047,  2000, 16861,  1998, 22091,\n",
      "         2015, 15775, 13231,  1012,  1045,  2699,  3015,  1037,  3642,  2008,\n",
      "         4152,  7696,  2013,  6202,  1011,  3193,  1998, 15389,  2015,  4449,\n",
      "         5834,  2006,  1996,  7755,  1012,  1045,  7718,  1996,  3642,  7246,\n",
      "         1998,  2673,  2499,  2986,  1025,  2021,  2043,  1045,  3231,  1996,\n",
      "         2717, 17928,  1045,  2131,  1996,  2206,  7561,  1024,  1063,  1004,\n",
      "        22035,  2102,  1025,  4471,  1004, 22035,  2102,  1025,  1024,  1004,\n",
      "        22035,  2102,  1025,  4394, 27280, 19204,  1004, 22035,  2102,  1025,\n",
      "         1065,  1045,  2275,  2039,  2026, 22496,  3081,  1004, 22035,  2102,\n",
      "         1025, 22091,  2015,  9530,  8873, 27390,  2063,  1004, 22035,  2102,\n",
      "         1025,  2004,  4541,  2182,  1024, 16770,  1024,  1013,  1013,  9986,\n",
      "         2015,  1012, 22091,  2015,  1012,  9733,  1012,  4012,  1013, 18856,\n",
      "         2072,  1013,  6745,  1013,  5310, 28582,  1013, 18856,  2072,  1011,\n",
      "         9530,  8873, 27390,  2063,  1011,  6764,  1012, 16129,  2072,  2036,\n",
      "         2580,  1037,  9530,  8873,  2290,  1012, 19067,  2102,  5371,  1999,\n",
      "         2026, 22091,  2015, 19622,  1998,  7039,  2026, 10906,  3081,  1004,\n",
      "        22035,  2102,  1025, 22091,  2015,  9530,  8873, 27390,  2063,  2131,\n",
      "         1004, 22035,  2102,  1025,  1998,  2027,  2020,  2986,  1012,  1996,\n",
      "         5950,  3853,  1999,  1996,  2927,  2499,  2205,  1025,  2061,   102])\n",
      "[CLS] aws chalice local works but not chalice deployi am pretty new to coding and aws chalice. i tried writing a code that gets messages from trading - view and executes orders depending on the signals. i tested the code locally and everything worked fine ; but when i test the rest api i get the following error : { & quot ; message & quot ; : & quot ; missing authentication token & quot ; } i set up my credentials via & quot ; aws configure & quot ; as explained here : https : / / docs. aws. amazon. com / cli / latest / userguide / cli - configure - files. htmli also created a config. txt file in my aws folder and checked my settings via & quot ; aws configure get & quot ; and they were fine. the index function in the beginning worked too ; so [SEP]\n",
      "Data: Targets tensor([0., 1., 0., 0., 0., 0., 1.])\n",
      "Data IDs  tensor([  101,  1004, 22035,  2102,  1025,  3685,  7170,  3128, 11336,  1004,\n",
      "         1001,  4464,  1025, 19888,  2080,  1012, 27715,  1012,  1035,  6315,\n",
      "         1035, 14925,  2497,  1004,  1001,  4464,  1025,  1024,  2667,  1004,\n",
      "         1001,  4464,  1025,  1035,  6315,  1035, 14925,  2497,  1012, 18133,\n",
      "        22123,  8747,  1011,  4229,  1011,  1060, 20842,  1035,  4185,  1011,\n",
      "        11603,  1011, 27004,  1012,  2061,  1004,  1001,  4464,  1025,  2043,\n",
      "         2770, 23375,  2006, 18750,  1017,  1012,  1022,  2072,  2572,  2667,\n",
      "         2026,  2070, 21999,  1998,  6412,  2007, 18750,  1998,  2667,  2000,\n",
      "         2448,  2009,  2006, 23375,  1012,  1045,  2572,  2893,  1996,  2917,\n",
      "         7561,  1024,  1063,  1004, 22035,  2102,  1025,  7561,  7834,  3736,\n",
      "         3351,  1004, 22035,  2102,  1025,  1024,  1004, 22035,  2102,  1025,\n",
      "         3685,  7170,  3128, 11336,  1005, 19888,  2080,  1012, 27715,  1012,\n",
      "         1035,  6315,  1035, 14925,  2497,  1005,  1024,  2667,  1005,  1035,\n",
      "         6315,  1035, 14925,  2497,  1012, 18133, 22123,  8747,  1011,  4229,\n",
      "         1011,  1060, 20842,  1035,  4185,  1011, 11603,  1011, 27004,  1012,\n",
      "         2061,  1005,  1024,  1013, 13075,  1013,  4708,  1013, 19888,  2080,\n",
      "         1013, 21183,  4014,  1013,  1012,  1012,  1013, 27715,  1013,  1035,\n",
      "         6315,  1035, 14925,  2497,  1012, 18133, 22123,  8747,  1011,  4229,\n",
      "         1011,  1060, 20842,  1035,  4185,  1011, 11603,  1011, 27004,  1012,\n",
      "         2061,  1024,  3685,  2330,  4207,  4874,  5371,  1024,  2053,   102])\n",
      "[CLS] & quot ; cannot load native module & # 39 ; crypto. cipher. _ raw _ ecb & # 39 ; : trying & # 39 ; _ raw _ ecb. cpython - 38 - x86 _ 64 - linux - gnu. so & # 39 ; when running lambda on python 3. 8i am trying my some encryption and description with python and trying to run it on lambda. i am getting the below error : { & quot ; errormessage & quot ; : & quot ; cannot load native module'crypto. cipher. _ raw _ ecb': trying'_ raw _ ecb. cpython - 38 - x86 _ 64 - linux - gnu. so': / var / task / crypto / util /.. / cipher / _ raw _ ecb. cpython - 38 - x86 _ 64 - linux - gnu. so : cannot open shared object file : no [SEP]\n",
      "Data: Targets tensor([0., 0., 1., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for datax in testing_loader:\n",
    "#   print(\"Data: \", datax) # ids, mask, token_type_ids, targets\n",
    "  print(\"Data IDs \", datax['ids'][0])\n",
    "#   print(\"Waveform: {}\\nSample rate: \\nLabels: \".format(datax[0] )) # , datax[1])) # , datax[2]))\n",
    "  print(tokenizer.decode(datax['ids'][0]))\n",
    "  print(\"Data: Targets\", datax['targets'][0])\n",
    "\n",
    "#   print(\"Data: \", datax['ids'][2])\n",
    "\n",
    "  if i == 1:\n",
    "      break\n",
    "  i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-48-f1d5dff74306>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-48-f1d5dff74306>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    --STOP--\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "--STOP--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('csv', data_files='softrain5Ktrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = dataset['train'].train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_test['train']\n",
    "test_dataset = train_test['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can inspect the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start the training we need to tokenize the data save it in S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://discuss.huggingface.co/t/fine-tune-for-multiclass-or-multilabel-multiclass/4035/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['ymlblabels'], padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset =  train_dataset.rename_column(\"ymlblabels\", \"labels\")\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset = test_dataset.rename_column(\"ymlblabels\", \"labels\")\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_prefix_sentiment = 'datasets/stackoverflow-nlp-hf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()  \n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sagemaker_session_bucket}/{s3_prefix_sentiment}/train'\n",
    "train_dataset.save_to_disk(training_input_path,fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sagemaker_session_bucket}/{s3_prefix_sentiment}/test'\n",
    "test_dataset.save_to_disk(test_input_path,fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters_sentiment={'epochs': 1,\n",
    "                 'train_batch_size': 8,\n",
    "                 'model_name': model_name\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator_sentiment = HuggingFace(entry_point='train.py',\n",
    "                                    source_dir='./scripts',\n",
    "                                    instance_type='ml.p3.2xlarge',\n",
    "                                    instance_count=1,\n",
    "                                    role=role,\n",
    "                                    transformers_version='4.6',\n",
    "                                    pytorch_version='1.7',\n",
    "                                    py_version='py36',\n",
    "                                    hyperparameters=hyperparameters_sentiment,\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator_sentiment.fit({'train': training_input_path, 'test': test_input_path}, \n",
    "                                    wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/analytics-vidhya/multi-label-text-classification-using-transformers-bert-93460838e62b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_sentiment = huggingface_estimator_sentiment.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    wait=False,\n",
    "    endpoint_name=\"turkish-sentiment-endpoint\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only required to create a predictor from an already deployed model\n",
    "predictor_sentiment = HuggingFacePredictor('turkish-sentiment-endpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text: \"This is a pretty bad product, I wouldn't recommend this to anyone\"\n",
    "sentiment_input= {\"inputs\": \"Bu oldukça kötü bir ürün, bunu kimseye tavsiye etmem\"}\n",
    "predictor_sentiment.predict(sentiment_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input text: \"I love this shampoo, it makes my hair so shiny\"\n",
    "sentiment_input= {\"inputs\": \"Bu şampuanı seviyorum, saçlarımı çok parlak yapıyor\"}\n",
    "predictor_sentiment.predict(sentiment_input)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.6-gpu-py36-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
